[
  {
    "path": "posts/2022-05-05-using-python-and-streamlit-to-dominate-wordle/",
    "title": "Using Python and Streamlit to Dominate Wordle",
    "description": "We walk through the steps of using Python and the Streamlit dashboard library to build a data driven app to help play the puzzle game Wordle.",
    "author": [
      {
        "name": "Logan Lossing",
        "url": {}
      }
    ],
    "date": "2022-03-05",
    "categories": [
      "python",
      "Data Science",
      "dashboard"
    ],
    "contents": "\nWordle is a popular word-based game where the ultimate goal is to guess a specific word. Typically, the game is played with 5 letter words but variations exist.\nYou are required to make guesses of the target word using valid English words of the appropriate length, usually 5 letters in length.\nFor each guess you are provided with the following information:\nthe letter turns green if the target word has that specific letter in that particular location in the word\nthe letter turns yellow if the target word contains that specific letter but in the target word, the letter appears in a different position\nthe letter turns grey if the target word does not contain that specific letter\n\n\n\nFigure 1: Screenshot from an New York Times Daily Wordle Game\n\n\n\nIn the screenshot, we can see that the first guess was ‚ÄòDEATH‚Äô and since the E and the H are both YELLOW then we know the target word contains E and H but the E is not in the second position and the H is not in the fifth position. Further since the D, A, and T are grey, we know the target word does not contain any of them.\nThe next guess was ‚ÄòETHER‚Äô and we now see that there are green E and R meaning that those letters are in the correct position. The H is yellow so we know that the H in the target word is not in the third location. The grey T shouldn‚Äôt suprise us, we already know from the first guess that no T existed in the target word and its presence in the second guess constitutes a waste of a guess. The second grey E signifies that the target word doesn‚Äôt have a second E in it other than the one that lives in the fourth location.\nFrom this example, we can determine that there are four pieces of information to keep track of:\nLetters we know are contained in the target word. We could store such values in a list. Example: contains_list = ['E', 'H', 'R']\nLetters we know are not contained in the target word. We could store such values in a list. Example: does_not_contain_list = ['D', 'A', 'T']\nLetters we know can‚Äôt be in specific positions in the target word. For a specific position, there may be many known non-valid letters. We could store such values in a dictionary using the positions as keys and a list of non-valid letters as values. position_not_known_dict = {2: ['E'], 3:['H'], 5:['H']}\nLetters we know to be in specific positions in the target word. For a specific position, there is only one possible valid letter. We could store such values in a dictionary using the positions as keys and a single valid letter as a value. Example: position_known_dict = {4: 'E', 5:'R'}\nFor each word guessed, we can update these 4 data structures to capture all of the information about what is allowed for the target word.\nBuild a Wordle Helper Function\nData\nThe first thing we need to generate are guesses. Each guess needs to be a valid English word of the right length.\nThe Kaggle Data contains ‚Äúthe counts of the 333,333 most commonly-used single words on the English language web, as derived from the Google Web Trillion Word Corpus.‚Äù One nice feature of these data is that there is a count column which we can use as a proxy for word popularity. Generally, the target word is most likely to be relatively close to the top of a table, sorted in decending order by count, containing the valid list of words currently meeting the known criteria.\nTo make these data a little more helpful for a Wordle Helper app, we can add a few helpful fields:\nnumber of letters per word to easily grab words of the right length\nnumber of distinct letters per word to perhaps choose words with more distinct letters to maximize the information we receive from our guess\nThis processing only needs to happen once. The code chuck below shows the contents of ETL_dictionary.py.\n\nimport pandas as pd\n\n# Load data with word, word freq over trillion word coupus\nword_dictionary = pd.read_csv(\"./data/unigram_freq.csv\")\n\n# remove null words\nword_dictionary = word_dictionary[word_dictionary.word.notnull()]\n\n# create column with word lengths\nword_dictionary['word_length'] = word_dictionary.word.str.len().astype(int)\nword_dictionary['distinct_letters'] = word_dictionary.word.apply(lambda x: len(set(x)))\n\n# export transformed data\nword_dictionary.to_csv(\"./data/transformed_dictionary.csv\", index = False)\n\nProgram Logic\nWith the right data available, we need to create a function that help to filter the the words to the most likely valid words.\nThe function below takes in all of the necessary inputs and eliminates words from the passed english_dictionary (the data we prepared previously) that are found to be impossible given the information we have discovered.\n\ndef get_possible_words(\n    english_dictionary,\n    word_length = 5, \n    contains_list = [], \n    does_not_contain_list = [], \n    position_known_dict = {}, \n    position_not_known_dict = {}):\n    \n    '''\n        english_dictionary - dictionary containing word and word metadata\n        word_length - integer indicating the word length for the puzzle (commonly 5 or 6)\n        contains_list - list of letters known to be in the target word (YELLOW or GREEN)\n            ex. ['a', 't']\n        does_not_contain_list - list of letters known to not be in target word (GREY)\n            ex. ['h']\n        position_known_dict - dictionary with letter postions as keys (GREEN)\n            ex. {1:'a', '5:'t'}\n        position_not_known_dict - dictionary with letter position as keys and values\n            in a list since there may be many known non-letters for a given position\n            ex. {1:['b', 'c'], 4: ['t]}\n    '''\n    \n    # subset to desired length\n    english_dictionary = english_dictionary[(english_dictionary.word_length == word_length)]\n\n    # remove impossible words\n    if contains_list:\n\n        for letter in contains_list:\n            english_dictionary = english_dictionary[\n                (english_dictionary.word.str.contains(letter, regex = True))\n                ]\n\n    if does_not_contain_list:\n\n        for letter in does_not_contain_list:\n            english_dictionary = english_dictionary[\n                (~english_dictionary.word.str.contains(letter, regex = True))\n                ]\n\n    if position_known_dict:\n\n        for key, value in position_known_dict.items():\n             english_dictionary = english_dictionary[\n                (english_dictionary.word.str.slice(start = key-1, stop = key, step = 1) == value)\n                ]\n\n    if position_not_known_dict:\n\n        for key, value in position_not_known_dict.items():\n            for letter in value:\n                english_dictionary = english_dictionary[\n                    (english_dictionary.word.str.slice(start = key-1, stop = key, step = 1) != letter)\n                    ]\n\n    print(f\"There are currently {len(english_dictionary.index)} possible words\")\n    print(f\"Here are the top 50 based on the information provided:\")\n    print(english_dictionary.head(50))\n\n    return english_dictionary\n\nOne downside to this function is that it requires us to rerun the function with each guess and type in all the information. Maybe we can make the function interactive by wrapping the functionality up into a dashboard?\nBuidling a Simple Streamlit App\nStreamlit is a very simple framework for quickly making data based web applications. The main thing to decide are what we want to control.\nThe main function get_possible_words() contains the following arguments that should be turned into interactive inputs in the app. The list of available widgets are here.\nword_length - a slider would work great for this. We only set this parameter once at the beginning of the game and leave it. Just for fun, let‚Äôs allow words as short as 1 and as long as 20.\ncontains_list - a multiselect works great for this. We will have a list of possible values (letters of the alphabet) for the user to select from and the selected values will populate the list\ndoes_not_contain_list = a multiselect works great for this. We will have a list of possible values (letters of the alphabet) for the user to select from and the selected values will populate the list\nposition_known_dict - a selectbox for each letter position would work, then we will be able to select only one value for each letter position. We need to dynamically add as a function of word_length.\nposition_not_known_dict - a multiselect works great for this but we will need one for each letter position. We need to dynamically add as a function of word_length.\nOnce the input controls are determined, we need to determine what output to create. For this app this is easy‚Ä¶we want to display the data meeting the requirements determined by the inputs. The data will be the main display of the app, along with some commentary and useful links.\nThe code for the Streamlit App is available below:\n\nimport streamlit as st\nimport pandas as pd\nfrom get_possible_words import *\n\nst.set_page_config(\n    page_title=\"Wordle Helper\", \n    page_icon=\"üìä\", \n    initial_sidebar_state=\"expanded\"\n)\n\n# Load data with word, word freq over trillion word coupus\nword_dictionary = pd.read_csv(\"./data/transformed_dictionary.csv\")\n\n# Set helper variables\nalphabet = ['', 'a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z']\nVERBOSE = False\n\nst.write(\n    \"\"\"\n# Wordle Helper\n*data + python to the rescue!*\nUse the sidebar filters to input information about the target word.\nEach guess at the target word provides new information about:\n+ letters that are contained in the target word [YELLOW and GREEN]\n+ letters that are not contained in the target word [GREY]\n+ letters in target word whose position are known [GREEN]\n+ letters in target word whose position is forbidden in a particular position [YELLOW]\nThe table below contains words meeting the selected criteria **sorted by how common the word is in common usage**.\nYou can use your best judgement about what word to actually use as your next guess but generally you will have\nvery good results just by always choosing the most common word meeting the criteria selected.\n## Table of Words Meeting Criteria\n\"\"\"\n)\n\nst.sidebar.write(\n    \"\"\"\n    # Parameters:\n\"\"\"\n)\n\nnumber = st.sidebar.slider(\n    'How many letters in the puzzle?',\n    min_value = 1,\n    max_value = 20,\n    value = 5,\n    step = 1)\n\n# get letters known to be in the target word\nknown_letters = st.sidebar.multiselect(\n     'What letters are known to be included in the target word?',\n     alphabet,\n     [])\n\n# get letters known to not be in the target word\nknown_nonletters = st.sidebar.multiselect(\n     'What letters are known to NOT be included in the target word?',\n     alphabet,\n     [])\n\n# get known position information\nknown_positions = {index:'' for index in range(1, number + 1)}\nfor pos in known_positions:\n    known_positions[pos] = st.sidebar.selectbox(\n        f\"What letter is known for string index {pos}?\",\n        alphabet,\n        key = 'known' + str(pos))\n\n# get known non-position information\nknown_nonpositions = {index:[] for index in range(1, number + 1)}\nfor pos in known_nonpositions:\n    \n    known_nonpositions[pos] = st.sidebar.multiselect(\n        f\"What letter(s) are forbidden for string index {pos}?\",\n        alphabet,\n        key = 'unknown' + str(pos))\n\n\n# convert the input data structures into form originally intended for get_possible_words()\nknown_position_dict = {}\nfor index in range(1, number + 1):\n    if known_positions[index] != '':\n        known_position_dict[index] = known_positions[index]\n\nknown_nonpositions_dict = {}\nfor index in range(1, number + 1):\n    if known_nonpositions[index] != []:\n        known_nonpositions_dict[index] = known_nonpositions[index]\n\n\ndictionary_data = get_possible_words(\n    word_dictionary,\n    word_length = number,\n    contains_list = known_letters,\n    does_not_contain_list = known_nonletters,\n    position_known_dict = known_position_dict, \n    position_not_known_dict = known_nonpositions_dict\n    )\nst.write(dictionary_data, height = 500)\n\nst.markdown(\"The data used is from [Kaggle](https://www.kaggle.com/datasets/rtatman/english-word-frequency).\")\nst.markdown(\"The code to build this app are located [github](https://github.com/nagol/wordle_helper).\")\n# For debugging\n#\"st.session_state object\", st.session_state\n\nif VERBOSE:\n    st.write('Puzzle word size: ', number)\n    st.write('Current known letters are:', ', '.join(known_letters))\n    st.write('Current known non-letters are:', ', '.join(known_nonletters))\n    \n    for pos in range(1, number + 1):\n        st.write(f\"The letter in position {pos} is known to be:\", known_positions[pos])\n        st.write(f\"The letter in position {pos} is forbidden to be:\", ', '.join(known_nonpositions[pos]))\n\nPublishing the App\nThe app can be easily deployed for free using the Streamlit Cloud.\nAfter making an account you can deploy the app directly from Github.\nLink to Final Streamlit App\n\n\n\nFigure 2: Screenshot from Final Wordle Helper App\n\n\n\n\n\n\n",
    "preview": "posts/2022-05-05-using-python-and-streamlit-to-dominate-wordle/wordle_game_screenshot.png",
    "last_modified": "2022-05-06T05:52:58-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-05-05-git-scraping-police-logs-with-github-actions/",
    "title": "Git-Scraping Police Logs with Github Actions",
    "description": "A example of a fully automated data pipeline using Github actions and Python to build an interesting data set over time.",
    "author": [
      {
        "name": "Logan Lossing",
        "url": {}
      }
    ],
    "date": "2021-09-15",
    "categories": [
      "python",
      "Git",
      "Data Engineering"
    ],
    "contents": "\nI have become increasingly interested in publicly available data and what insights such data can provide. For example, the San Luis Obispo Police Department in San Luis Obispo, California publishes records about each call they respond to. The department publishes their data a few times a week but provides no historical data.\nIn order to be able to see any interesting trends in such data we need to come up with a way to easily and automatically collect the data.\n\n\n\nFigure 1: Figure from Github Repo: https://github.com/nagol/SLOPD_activity showing trend in Noise related calls\n\n\n\nGit-Scraping\nGit-scraping is a fantastic solution for the automated scraping and storing of data. The term/concept was pioneered by Simon Willson in his article. His article is fantastic; short, and easy to follow with plenty of examples.\nThe goal for this post was to try to write my own git-scraper from scratch. The resulting code for the project is available on Github\nGetting the Data\nThe San Luis Obispo Police Department publishes their data at the following link SLOPD Data. According to the website, the data is updated by 3PM Monday - Thursday. The Monday data contains all of the weekend data as well.\nNormally when we say ‚Äòscraping data‚Äô we mean using tools like R‚Äôs rvest or Python‚Äôs beautiful soup or scrapy to extract data from the DOM of a webpage. In this case the data is actually posted as an embedded flat-file text document.\nThe ‚Äòscraper‚Äô in this case downloads the text file, breaks the file up into ‚Äòchunks‚Äô by exploiting the formatting of the data, iterates over the chunks (each of which represents a SLOPD incident), and parses out the data. I tested the scraper for a few days locally to make sure the data could be parsed correctly before trying to any automating.\nThe Python code to parse this file is located here.\nAutomating with Github Actions\nThe script works great for manually gathering data but I need this thing to run automatically. Further, we don‚Äôt want to introduce any data duplication.\nThere are many ways to handle this situation:\nWe could write a cron job. What happens though if your machine doesn‚Äôt have internet access or is not on? We can‚Äôt recover the historical data, it would be lost forever.\nWe could setup an AWS EC2 instance and run our script there. Totally doable but takes time to setup and may involve re-occurring charges.\nEnter Github Actions.\nWe can automate our workflow by creating a YAML file that describes the workflow.\nThe code chunk below contains a copy of this file. The basic pieces are:\nSpecifying when the scripts should run. Since I know when the department updates their data, the scraper only run once a day Monday - Thursday.\nSpecifying the runtime environment.\nwe specify to checkout the repo code\nwhat type of machine to run the code on (Ubuntu)\nhow to configure the Python environment (version, packages to install)\n\nListing the tasks to complete.\nrun slopd_scraper.py\nrun analysis.py\n\nCommit changes with a timestamp in the commit message.\n\n\nname: scraper-slopd\n\non:\n  schedule:\n    - cron: '26 23 * * MON,TUE,WED,THU'\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n    steps:\n\n      - name: checkout repo content\n        uses: actions/checkout@v2 # checkout the repository content to github runner\n\n      - name: setup python\n        uses: actions/setup-python@v2\n        with:\n          python-version: '3.7.7'\n          \n      - name: install python packages\n        run: |\n          python -m pip install --upgrade pip\n          pip install -r requirements.txt\n          \n      - name: execute py script\n        run: python scripts/slopd_scraper.py\n        \n      - name: execute py script2\n        run: python scripts/analysis.py\n          \n      - name: commit files\n        run: |\n          git config --local user.email \"action@github.com\"\n          git config --local user.name \"GitHub Action\"\n          git add -A\n          timestamp=$(date -u)\n          git commit -m \"update with latest data, create new plots: ${timestamp}\" || exit 0\n          \n      - name: push changes\n        uses: ad-m/github-push-action@v0.6.0\n        with:\n          github_token: ${{ secrets.GITHUB_TOKEN }}\n          branch: main\n\n\n\n\n",
    "preview": "posts/2022-05-05-git-scraping-police-logs-with-github-actions/final_time_series.png",
    "last_modified": "2022-05-06T06:44:02-07:00",
    "input_file": "git-scraping-police-logs-with-github-actions.utf8.md"
  },
  {
    "path": "posts/2021-02-16-tidy-tuesday-web-du-bois/",
    "title": "Tidy Tuesday - Recreating Works of W.E.B. Du Bois",
    "description": "The goal of this week's Tidy Tuesday challenge was to celebrate the data visualization legacy of W.E.B DuBois by recreating a visualization from the 1900 Paris Exposition.",
    "author": [
      {
        "name": "Logan Lossing",
        "url": {}
      }
    ],
    "date": "2021-02-16",
    "categories": [
      "Data Visualization",
      "R"
    ],
    "contents": "\nW.E.B Du Bois\nIn this week‚Äôs Tidy Tuesday, the goal was to try and recreate some of the works of W.E.B. Du Bois.\nI decided to try to see what I could do with the fourth challenge. The original shows the proportion of Black Americans that were freemen vs.¬†slaves over the course of the 19th century.\n.\n\nShow code\nlibrary(tidytuesdayR)\nlibrary(tidyverse)\nlibrary(ggtext)\nlibrary(showtext)\n\nfreed_slaves <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-02-16/freed_slaves.csv')\n\n\n\n\nShow code\nfont_add_google(name = \"Rambla\")\nshowtext_auto()\n\ngreen_color <- '#597D35'\noat_color <- '#E0D6B4'\n\nplot <- freed_slaves %>%\n    ggplot(aes(x = Year, y = Slave)) +\n    \n    geom_ribbon(aes(ymin = 0, ymax = Slave)) +\n    geom_ribbon(aes(ymin = Slave, ymax = 100), fill = green_color) +\n    guides(color = FALSE, fill = FALSE) +\n  \n    labs(\n        title = NULL,\n        x = NULL,\n        y = NULL\n    ) +\n    coord_cartesian(ylim = c(0, 100), clip = 'off') +\n    coord_cartesian(xlim = c(1790, 1870), clip = 'off') +\n    scale_x_continuous(position = \"top\", breaks  = seq(1790, 1870, 10)) +\n    scale_y_continuous(labels = NULL, breaks = NULL) +\n  \n    geom_text(data = freed_slaves %>% filter(Year > 1790, Year < 1870),\n        aes(x = Year, y = Slave + 2, label = glue::glue(\"{Free}%\")), \n        # hjust = 0,\n        size = 5,\n        color = 'black') + \n    annotate('text', x = 1825, y = 50, \n             label = glue::glue(\"SLAVES\"), color = 'white', size = 10) +\n    annotate('text', x = 1825, y = 45, \n             label = glue::glue(\"ESCLAVES\"), color = 'white', size = 10) +\n    annotate('text', x = 1825, y = 97.5, \n             label = glue::glue(\"FREE - LIBRE\"), color = 'black', size = 6, fontface = 'bold') +\n    annotate(\"text\", label = \"100 %\", x = 1870, y = 92,\n              color = \"#101010\", size = 5) +\n    annotate(\"text\", label = \"8 %\", x = 1790, y = 94,\n             color = \"#101010\", size = 5) +\n  \n    theme(\n        axis.ticks = element_blank(),\n        legend.title = element_blank(),\n        axis.text.x.top=element_text(\n          size=14, \n          face = 'bold', \n          margin = margin(20,0,0,0), \n          vjust = -8),\n        panel.background = element_rect(fill = oat_color, colour = oat_color),\n        panel.border = element_blank(),\n        panel.grid.major.x = element_blank(),\n        panel.grid.minor.x = element_blank(),\n        plot.background = element_rect(fill = oat_color)\n    )\n\nplot\n\n\n\n\nFuture\nAll-in-all, I am fairly happy with the result. I clearly could fix a few things:\ntypography - find fonts that match a little better, needs a heavier font weight\ntexture - would be nice to match the brush strokes that can be seen on the original\nforeign text - need to learn how to deal with foreign languages to add accents\n\n\n\n",
    "preview": "posts/2021-02-16-tidy-tuesday-web-du-bois/tidy-tuesday-web-du-bois_files/figure-html5/plot-1.png",
    "last_modified": "2021-02-16T17:29:56-08:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-02-08-tidy-tuesday-wealth-income/",
    "title": "Tidy Tuesday - Wealth & Income",
    "description": "Visualizing disparities in Wealth & Income in the US over the last 40 years as\npart of Tidy Tuesday, the weekly social data project in R.",
    "author": [
      {
        "name": "Logan Lossing",
        "url": {}
      }
    ],
    "date": "2021-02-08",
    "categories": [
      "Data Visualization",
      "R"
    ],
    "contents": "\n\nShow code\n# Libs\nlibrary(tidyverse)\nlibrary(gganimate)\nlibrary(tidytuesdayR)\nlibrary(ggtext)\nlibrary(ggdark)\n\n# Load list of tbls\ntuesdata <- tidytuesdayR::tt_load('2021-02-09')\n\nincome_tbl <- tuesdata$income_distribution\nlifetime_earn_tbl <- tuesdata$lifetime_earn\n\n\n\nIncome Distribution\nI have been meaning to spend more time with gganimate and these data provided and opportunity.\nWhat extra value does gganimate versus regular plots?\nThe ability to draw the readers attention and make the data come alive.\nThis is especially true when asking the reader a question in the title. You are providing a task to be accomplished. By using animation, we can direct focus to the where the action is.\nIn this particular example, I think it is really powerful to see the proportion of high-income white individuals shoot up while the proportion of high-income Hispanic and Black individuals climbs very slowly.\n\nShow code\nincome_levels <- c(\n  \"Under $15,000\",\n  \"$15,000 to $24,999\",\n  \"$25,000 to $34,999\",\n  \"$35,000 to $49,999\",\n  \"$50,000 to $74,999\",\n  \"$75,000 to $99,999\",\n  \"$100,000 to $149,999\",\n  \"$150,000 to $199,999\",\n  \"$200,000 and over\"\n)\n\n# Define colors for race\ncolor_group_1 <- \"#8900ff\"\ncolor_group_2 <- \"#fff205\"\ncolor_group_3 <- \"#ff1178\"\n\n\nplot <- income_tbl %>% \n  filter(race %in% c(\"Black Alone\", \"White Alone\", \"Hispanic (Any Race)\")) %>%\n  filter(year >= 1980) %>%\n  filter(income_bracket %in% c('Under $15,000','$75,000 to $99,999','$200,000 and over')) %>%\n  mutate(race = case_when(\n    race == 'Black Alone' ~ 'Black',\n    race == 'White Alone' ~ 'White',\n    race == 'Hispanic (Any Race)' ~ 'Hispanic'\n  )) %>%\n  mutate(income_dist_percent = income_distribution / 100) %>%\n  mutate(income_bracket = factor(income_bracket, levels = income_levels)) %>%\n  \n  ggplot(aes(x = year, y = income_dist_percent, color = race, group = race)) +\n    geom_path(size = 1.2, alpha = 0.7) +\n    coord_cartesian(clip = 'off') +\n    scale_y_continuous(position = 'right', name = ' ', labels = scales::percent) +\n    scale_color_manual(values = c(color_group_1, color_group_2, color_group_3)) +\n    labs(\n      title = \"Is income disparity by race disappearing over time or getting worse?\",\n      subtitle = glue::glue(\"Non-White households are overrepresented in lower income brackets and are not experiencing \\n  similar increases in representation in higher income brackets as White households.\"),\n      x = NULL\n    ) +\n  \n    transition_reveal(along = year) +\n    \n    facet_grid(\n      rows = vars(income_bracket), \n      switch = 'y', \n      labeller = labeller(income_bracket = label_wrap_gen(width = 10))\n    ) +\n    \n    dark_theme_minimal() +\n    theme(legend.position = 'top',\n          strip.text.y.left = element_text(angle = 0),\n          legend.title = element_blank(),\n          plot.background = element_rect(color = 'black'),\n          panel.grid.major.x = element_line(color = \"#44475a\", size = 0.3),\n          panel.grid.major.y = element_line(color = \"#44475a\", size = 0.3),\n          panel.spacing.y = unit(2, \"lines\"))\n  \n\nanimate(plot, height = 6, width = 9, units = \"in\", res = 200)\n\n\nShow code\nanim_save(\"income_disparity.gif\")\n\n\n\nIncome Distribution 2 and The Bug I Need to Figure Out\nI wanted to make the previous plot even easier to follow by printing out the percentage for the current year at the current end of the time series. At the moment, I have am having an issue with this plot as the plotted text and points from the first frame persist through the animation.\n\nShow code\nincome_levels <- c(\n  \"Under $15,000\",\n  \"$15,000 to $24,999\",\n  \"$25,000 to $34,999\",\n  \"$35,000 to $49,999\",\n  \"$50,000 to $74,999\",\n  \"$75,000 to $99,999\",\n  \"$100,000 to $149,999\",\n  \"$150,000 to $199,999\",\n  \"$200,000 and over\"\n)\n\n# Define colors for race\ncolor_group_1 <- \"#8900ff\"\ncolor_group_2 <- \"#fff205\"\ncolor_group_3 <- \"#ff1178\"\n\n\nplot <- income_tbl %>% \n  filter(race %in% c(\"Black Alone\", \"White Alone\", \"Hispanic (Any Race)\")) %>%\n  filter(year >= 1980) %>%\n  filter(income_bracket %in% c('Under $15,000','$75,000 to $99,999','$200,000 and over')) %>%\n  mutate(race = case_when(\n    race == 'Black Alone' ~ 'Black',\n    race == 'White Alone' ~ 'White',\n    race == 'Hispanic (Any Race)' ~ 'Hispanic'\n  )) %>%\n  mutate(income_dist_percent = income_distribution / 100) %>%\n  mutate(income_bracket = factor(income_bracket, levels = income_levels)) %>%\n  \n  ggplot(aes(x = year, y = income_dist_percent, color = race, group = race)) +\n    geom_path(size = 1.2, alpha = 0.7) +\n    coord_cartesian(clip = 'off') +\n    scale_y_continuous(position = 'right', name = ' ', labels = scales::percent) +\n    scale_color_manual(values = c(color_group_1, color_group_2, color_group_3)) +\n    labs(\n      title = \"Is income disparity by race disappearing over time or getting worse?\",\n      subtitle = glue::glue(\"Non-White households are overrepresented in lower income brackets and are not experiencing \\n  similar increases in representation in higher income brackets as White households.\"),\n      x = NULL\n    ) +\n    \n    geom_point(size = 2) + \n    geom_text(aes(x = year + 1, label = scales::percent(income_dist_percent, accuracy = 0.1)), hjust = 0) + \n    transition_reveal(along = year) +\n    \n    facet_grid(\n      rows = vars(income_bracket), \n      switch = 'y', \n      labeller = labeller(income_bracket = label_wrap_gen(width = 10))\n    ) +\n    \n    dark_theme_minimal() +\n    theme(legend.position = 'top',\n          strip.text.y.left = element_text(angle = 0),\n          legend.title = element_blank(),\n          plot.background = element_rect(color = 'black'),\n          panel.grid.major.x = element_line(color = \"#44475a\", size = 0.3),\n          panel.grid.major.y = element_line(color = \"#44475a\", size = 0.3),\n          panel.spacing.y = unit(2, \"lines\"))\n  \nanimate(plot, height = 6, width = 9, units = \"in\", res = 200)\n\n\nShow code\nanim_save(\"income_disparity_buggy.gif\")\n\n\n\nAverage Lifetime Earnings\nFor this simple plot, I thought it would be nice to find put real differences in lifetime earnings on a ‚Äòfinancial goal‚Äô axis but keep the plot minimal overall.\n\nShow code\nlifetime_earn_wm <- lifetime_earn_tbl %>% \n  filter(gender == 'Men', race == 'White') %>%\n  pull()\n\ncolor_group_1 <- \"#d21404\"\ncolor_group_2 <- \"#ff6961\"\n\nwealth_plot <- lifetime_earn_tbl %>%\n  mutate(percent_white_male = (lifetime_earn - lifetime_earn_wm) / lifetime_earn_wm ) %>%\n  mutate(diffinc_white_male = lifetime_earn - lifetime_earn_wm) %>%\n  mutate(race = if_else(race == 'Hispanic any race', 'Hispanic', race)) %>%\n  mutate(category = str_glue(\"{race} \\n {gender}\")) %>%\n  filter(race != \"White\" | gender != \"Men\") %>%\n  arrange(percent_white_male) %>%\n  ggplot(aes(x = reorder(category, percent_white_male), y = percent_white_male, color = gender)) +\n  expand_limits(y = 0) +\n  geom_bar(stat = \"identity\", width = 0.01) + \n  geom_point(shape = 25) +\n  scale_color_manual(values = c(color_group_1, color_group_2)) +\n  labs(\n    title = \"Economic Disparity Charges Interest\",\n    subtitle = glue::glue(\"The average lifetime earnings in 2016 for White Men was {scales::dollar(lifetime_earn_wm)}. \\n By comparison, Women and BIPOC make significantly less...but by how much?\"),\n    x = NULL,\n    caption = \"Data: Urban Institute and the US Census.\"\n  ) + \n  guides(color = FALSE, fill = FALSE) +\n  dark_theme_minimal() +\n  scale_y_continuous(\n    \n    # first axis\n    name = \" \",\n    labels = scales::percent,\n    \n    # Add a second axis\n    sec.axis = sec_axis(\n      ~.*lifetime_earn_wm, \n      name=\" \",\n      breaks = c(0, -233000, -650000, -1300000), \n      labels = c(\"($0)\", \n                 glue::glue(\"Avg Cost to Raise Child \\n ($233,000)\"), \n                 glue::glue(\"Median Home Price \\n Los Angeles \\n ($650,000)\"),\n                 glue::glue(\"Median Home Price \\n San Francisco \\n ($1,300,000)\"))\n      )\n    ) + \n  \n  theme(\n      axis.ticks = element_blank(),\n      legend.title = element_blank(),\n      plot.background = element_rect(color = 'black'),\n      plot.caption = element_text(size = 8, lineheight = .9),\n      plot.caption.position = \"plot\",\n      plot.margin = margin(1, 2, 1, 1, unit = \"line\"),\n    ) \n\nwealth_plot\n\n\n\n\n\n\n\n",
    "preview": "posts/2021-02-08-tidy-tuesday-wealth-income/tidy-tuesday-wealth-income_files/figure-html5/avg_lifetime-1.png",
    "last_modified": "2021-02-16T13:49:48-08:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-22-the-page-rank-algorithm/",
    "title": "The Page Rank Algorithm",
    "description": "Google's Page Rank Algorithm was an early tool that helped determine the\nrelevance of search results.\n\nHow does the algorithm work? How can we implement the algorithm?\n\nIn this post, we investigate the mathematics behind the Page Rank algorithm and\nimplement several different methods for obtaining results using R.",
    "author": [
      {
        "name": "Logan Lossing",
        "url": {}
      }
    ],
    "date": "2021-01-22",
    "categories": [
      "R",
      "linear algebra",
      "probability"
    ],
    "contents": "\n\nContents\nThe Page Rank Problem: An Exploration\nSummary Statistics of the NCSU Data\n\nSolving the Page Rank Problem\nUsing a Dense Linear System Solver\nUsing a Simple Iterative Linear System Solver\nUsing a Dense Eigen-Solver\nUsing a Simple Iterative Eigen-Solver\n\nThe Results\nExtension of this Analysis to the Entire Internet\n\n\nOriginally published in 2014\nThe Page Rank Problem: An Exploration\nWhen searching for a particular keyword on the internet, we want our search engine to return \"relevant\" websites. There are many possible ways of defining what we mean by relevant, but the one that the Google founders came up with in the 1990‚Äôs was the Page Rank method.\nThe crux of the idea of the Page Rank method is that the \"importance\" of a particular web-page can be defined to be the number of web-pages that link to it. The authors at http://www.math.cornell.edu/~mec/Winter2009/RalucaRemus/Lecture3/lecture3.html did a great job in explaining what this means, and I borrow their explanation here.\n\n\"If we create a web page i and include a hyperlink to the web page j, this means that we consider j important and relevant for our topic. If there are a lot of pages that link to j, this means that the common belief is that page j is important. If on the other hand, j has only one backlink, but that comes from an authoritative site k, (like www.google.com, www.cnn.com, www.cornell.edu) we say that k transfers its authority to j; in other words, k asserts that j is important. Whether we talk about popularity or authority, we can iteratively assign a rank to each web page, based on the ranks of the pages that point to it.\"\n- Cornell Math Explorers Club Website\n\nUsing this idea of importance, we can turn a directed graph, one that represents the network structure of webpages (nodes are websites, edges are links), into a transition matrix for a Markov Chain. Then we can think of the most \"relevant\" websites as those with the highest probabilities in the stationary distribution of the Markov Chain.\nThere are, however, two issues that we need to adjust for. The first is that our graph may not be fully connected. That is, we are not guaranteed that a path exists to every website from every other website. This causes us to get stuck in one part of the graph and not be able to assess the importance of other parts of the graph. The second issue is ‚Äúdangling nodes,\" websites that have links to them but they do not link anywhere else. In order to adjust for this, we have the idea of a‚Äùrandom surfer.\"\nA \"random surfer\" is an entity that will randomly surf around the graph according to some basic rules. Let \\(r_{i}\\) be the out degree of webpage i. The out degree of a webpages is defined to be the number of webpages that it links to. When arriving to webpage i then they do one of the following things:\nIf \\(r_{i} > 0\\) then with probability p they uniformly chose a link on that page and with probability 1-p they choose a link uniformly from the space of n-pages\nIf \\(r_{i} = 0\\) then they choose a random page uniformly from the space of n-pages\nWe can notate this as follows.\nLet A be an (nxn) adjacency matrix where \\(a_{ij} = \\mathbb{1}(\\text{webpage i links to webpage j})\\)\nDefine p to be the probability that a random surfer follows a link on the current page (1-p is called the \"teleportation\" parameter)\n\\(r_{i}\\) be the out degree of webpage i. The out degree of a webpages is defined to be the number of webpages that it links to\nThe transition matrix P has entries \\(p_{ij} = \\mathbb{1}(r_{i}>0)\\Big (\\frac{pa_{ij}}{r_{i}} \\Big ) + \\mathbb{1}(r_{i}=0) \\Big ( \\frac{1-p}{n} \\Big )\\)\nIn matrix form, this can be written as:\n\\[P = pR^{+}A + z1_{n}^T\\]\nwhere, \\(R = \\text{diag}(r_{1},...,r_{n})\\) and \\(z_{j} = \\mathbb{1}(r_{i}>0)(\\frac{1-p}{n}) + \\mathbb{1}(r_{i}=0)(\\frac{1}{n})\\)\nGiven this Markov Chain setup, we can find the stationary distribution which corresponds to the ranks (probabilities) of each page.\nIt turns out that P is a positive row stochastic matrix and thus \\(P^T\\) is column stochastic. The Perron-Frobenius Theorem guarantees that any column stochastic matrix has the following properties:\n1 is an eigenvalue of multiplicity one\n1 is the largest eigenvalue, all others are smaller in modulus\nThere exists a unique eigenvector corresponding to 1 that has all positive entries and sums to 1\nThis theorem thus gives us a few interesting ways to look at this problem, yielding several possible methods of solution.\nThe first way to think about this is as an eigen-problem. We want to find the eigenvector corresponding to eigenvalue 1. Since this eigenvalue is multiplicity one then there is only one such eigenvector (up to a constant). That is, we want to find x such that \\(P^Tx = x\\). There are many possible ways to solve such and eigen-problem such as Singular-Value Decomposition(SVD) or iteratively using the Power-Method.\nThe second way to think about this problem is as a linear system of equations. \\(P^Tx = x \\iff (I-P^T)x = 0\\). We can solve such a linear system using ... or iteratively using the Jacobi Method, Gauss-Seidel, or Bi-Conjugate Gradient Methods.\nIn the rest of this document we explore these different methods of solving the Page Rank Problem using a data set of 500 webpages connected to the www.stat.ncsu.edu webpage. (Go Wolfpack!)\nSummary Statistics of the NCSU Data\nThe A.txt data set is a (500x500) adjacency matrix of the top 500 web-pages linking to the www.stat.ncsu.edu website.\nThe total number of edges is equal to \\(\\sum_{i}{\\sum_{j}{a_{ij}}} = 14,102\\).\nThe out degree of the matrix is \\(r_{i} = \\sum_{j}{a_{ij}}\\). The max out degree was 126, and the min was 0.\nThere were 97 dangling nodes.\nThese calculations were performed with the following code:\n\n\n\n\n\nadj.matrix <- read.table(path_to_dataA,header=FALSE,sep = \",\")\nlabels <- read.table(path_to_dataU,header=FALSE,sep = \",\",as.is=TRUE)\nadj.matrix <- as.matrix(t(adj.matrix))\nlabels<-as.matrix(labels)\n\n(number.of.pages <- dim(adj.matrix)[1])\n\n\n[1] 500\n\n(number.of.edges <- sum(adj.matrix))\n\n\n[1] 14102\n\n##  ri- the out degree = sum_j (Aij)\nout.degree <- apply(adj.matrix,1,sum)\n(min(out.degree))\n\n\n[1] 0\n\n(max(out.degree))\n\n\n[1] 126\n\n(number.of.dangling.nodes <- sum(out.degree==0))\n\n\n[1] 97\n\nin.degree <- apply(adj.matrix,2,sum)\n(max(in.degree))\n\n\n[1] 268\n\n(min(in.degree))\n\n\n[1] 1\n\nLet‚Äôs quickly take a second to look at the sparsity pattern in the adjacency matrix. This is accomplished using the function fields::image.plot() from the \"fields\" package. The code to make this plot is below.\n\n\n\nWe can see that the majority of the matrix is zeros, thus this is a sparse matrix. This structure can be exploited to speed up the solving of this system. We also see that many pages have nearly identical linking structures.\nSolving the Page Rank Problem\nFor the analyses that follow, we set the \"teleportation\" parameter to be .15, thus p=.85 The first few methods of solution will look at the problem from the perspective of a linear system, the second as an eigen-problem.\nBefore preceding, we need to read-in/create all the necessary variables and matrices. This is handled in the output below.\n\n\n\nUsing a Dense Linear System Solver\nWe are trying solve the linear equation \\((I-P^T)x = 0\\), thus \\(x \\in \\mathcal{N}(I-P^T)\\). The Perron-Frobenius Theorem guarantees that there is a single vector in the \\(\\mathcal{N}(I-P^T)\\), thus we need not worry about uniqueness. QR decomposition is going to make solving this system very easy.\nIf we perform a QR decomposition on \\((I-P^T)^T\\) with rank r, then the first r columns form an orthonormal basis for the column space of \\((I-P^T)^T\\) while the remaining n-r columns form an orthonormal basis for the null space of \\((I-P^T)\\). Since \\((I-P^T)^T\\) is only rank deficient by one, the last column of Q, when properly normalized, will be our solution.\nBelow is my implementation of this method and the first five elements of my solution vector.\n\n\nqr.i.minus.ptt <- qr(t(i.minus.pt))\nQ <- qr.Q(qr.i.minus.ptt)\nsolution.qr <- Q[,500]\nsolution.qr <- solution.qr/sum(solution.qr)\nsolution.qr[1:5]\n\n\n[1] 0.002136784 0.235939467 0.010670053 0.005944397 0.006703392\n\nA very similar method to solving this problem in almost the exact same fashion, would be to use the svd() function. Singular-Value Decomposition also allows us to find a basis for the null space of \\((I-P^T)^T\\), and this can be accomplished with the following code.\n\n\nsvd.P <- svd(i.minus.pt)\nsolution.svd <- svd.P$v[,500,drop=F]\nsolution.svd <- solution.svd/sum(solution.svd)\nsolution.svd[1:5]\n\n\n[1] 0.002136784 0.235939467 0.010670053 0.005944397 0.006703392\n\nUsing a Simple Iterative Linear System Solver\nNow we want to solve for the same vector as before, using an iterative solver. A very straight-forward method for doing this is the Jacobi method.\nBelow is my implementation of the Jacobi Method for this problem.\n\n\nJacobiSolver <- function(matrixA,b,init.guess,tol,max.iter=5000){\n  ####################################################################\n  ##  Want to solve linear system Ax = b using iterative Jacobi method\n  ##  Input:\n  ##  init.guess = best initial guess for x\n  ##  tol = the tolerance for convergence (looking at max difference)\n  ##  max.iter = stopping condition if convergence not reached\n  ##  Output:\n  ##  solution x with information about convergence\n  ####################################################################\n  if(prod(diag(matrixA)!=0)==0){\n    print(\"All diagonal elements must be non-zero\")\n    return(0)\n  }\n  L.plus.U <- matrixA\n  D <- diag(matrixA)\n  diag(L.plus.U) <- 0\n\n  x.old <- init.guess  ## Initial Conditions\n  x.ten <- init.guess\n  max.iterations <- max.iter\n  tolerance <- tol\n  count = 0\n  for(i in 1:max.iterations){\n    ##  Update\n    x.new <- -1*(1/D)*L.plus.U%*%x.old + (1/D)*b\n\n    ##  Check if converging\n    if(i%%10 == 0){\n      diff <- abs(x.new-x.ten)\n      x.ten <- x.new\n      if(max(diff)<tolerance){ print(noquote(\"Converged!\"));break;}\n    }\n\n    ##  Get ready for the next loop\n    x.old <- x.new\n    count <- count +1\n    if(i == max.iterations){print(\"Not-Converged\");return(0)}\n  }\n  solution.Jacobi <- x.new/sum(x.new)\n  print(noquote(paste(\"Iterative proceedure looped\",count,\"number of times.\")))\n  return(solution.Jacobi)\n}\n##  Make sure it works!\nx.old <- matrix(1/n,nrow=500,ncol=1)\nsolution.Jacobi <- JacobiSolver(i.minus.pt,rep(0,500),x.old,.000001)\n\n\n[1] Converged!\n[1] Iterative proceedure looped 29 number of times.\n\nsolution.Jacobi[1:5]\n\n\n[1] 0.002136784 0.235939467 0.010670053 0.005944397 0.006703392\n\nUsing a Dense Eigen-Solver\nAs an eigen-problem we are trying to find the eigen-vector associated with the maximum eigenvalue 1, which is of multiplicity 1. One great way to do this in \\(\\mathbb{R}\\) is with the eigen() function, which implements Full Symmetric Eigen-Decomposition (tri-diagonalization + QR with implicit shift). This function finds all eigenvalues of a matrix, and on request, will find the eigenvectors as well.\nFor this problem, we know we want to find the eigenvector corresponding to the eigenvalue 1. That is, we are trying to find x such that \\(P^Tx=x\\). This is accomplished with the following code.\n\n\nsolution <- abs(eigen(t(P),symmetric=FALSE)$vectors[,1,drop=F])\nsolution <- solution/sum(solution)\nsolution[1:5]\n\n\n[1] 0.002136784 0.235939467 0.010670053 0.005944397 0.006703392\n\nUsing a Simple Iterative Eigen-Solver\nThe Power Method is a very simple algorithm for finding the maximum eigenvalue and eigenvector for a matrix. We know by the Perron-Frobenius Theorem that 1 is the largest eigenvalue, thus applying this method to \\(P^T\\) will provide the solution to our problem.\nMy implementation of the Power Method for \\(P^T\\) is below.\n\n\nPowerMethodSolver <- function(matrixA,init.guess,tol,max.iter=5000){\n  ####################################################################\n  ##  Want to find large eigenvalue and associated eigenvector\n  ##  Ax = LAMBDAx\n  ##  Input:\n  ##  init.guess = best initial guess for x\n  ##  tol = the tolerance for convergence (looking at max difference)\n  ##  max.iter = stopping condition if convergence not reached\n  ##  Output:\n  ##  solution x with information about convergence\n  ####################################################################\n  x.old <- init.guess  ## Initial Conditions\n  x.ten <- x.old\n  max.iterations <- max.iter\n  tolerance <- tol\n  count = 0\n\n  for(i in 1:max.iterations){\n\n    ##  Calculate next iteration\n    x.new <- matrixA%*%x.old\n    x.new <- x.new/sqrt(sum(x.new^2))\n\n    ##  Check if converging\n    if(i%%10 == 0){\n      diff <- abs(x.new-x.ten)\n      x.ten <- x.new\n      if(max(diff)<tolerance){ print(\"Converged!\");break;}\n    }\n\n    ##  Update\n    x.old <- x.new\n    count <- count+1\n    if(i == max.iterations){print(\"Not-Converged\");return(0)}\n\n  }\n  solution.power <- x.new/sum(x.new)\n  print(noquote(paste(\"Method iterated\",count,\"number of times.\")))\n  return(solution.power)\n}\n##  Make sure it works!\nx.old <- matrix(1/n,nrow=500,ncol=1)  ## Initial Conditions\nsolution.power <- PowerMethodSolver(t(P),x.old,.000001)\n\n\n[1] \"Converged!\"\n[1] Method iterated 59 number of times.\n\nsolution.power[1:5]\n\n\n[1] 0.002136784 0.235939385 0.010670055 0.005944398 0.006703393\n\nThe Results\nNow that we have solved this problem in a number of different ways, we can answer the question that we set out to answer; what are the top 20 websites connected to the www.stat.ncsu.edu website?\nWe can find these websites with the highest stationary state probabilities with the following code.\n\n      Stationary Probabilities\n [1,] \"0.235939466951902\"     \n [2,] \"0.0211065334273055\"    \n [3,] \"0.0144917606322821\"    \n [4,] \"0.0140769056962011\"    \n [5,] \"0.0135504522670688\"    \n [6,] \"0.0110037883588661\"    \n [7,] \"0.010670053024285\"     \n [8,] \"0.0101356734130066\"    \n [9,] \"0.00975252510864476\"   \n[10,] \"0.00823415213171245\"   \n[11,] \"0.0078921509700482\"    \n[12,] \"0.00731078337779315\"   \n[13,] \"0.0069518473720118\"    \n[14,] \"0.00687005153079194\"   \n[15,] \"0.00670339199514352\"   \n[16,] \"0.00623291067709522\"   \n[17,] \"0.00594439722646046\"   \n[18,] \"0.00547885860901607\"   \n[19,] \"0.00534747062763832\"   \n[20,] \"0.00521543243231812\"   \n      Websites                                 \n [1,] \"http://www\"                             \n [2,] \"http://purl.org/rss/1.0/modules/content\"\n [3,] \"http://ns.adobe.com/xap/1.0\"            \n [4,] \"http://ncsu.edu\"                        \n [5,] \"http://instagram.com/ncstate\"           \n [6,] \"http://www.\"                            \n [7,] \"http://www.ncsu.edu\"                    \n [8,] \"http://purl.org/dc/elements/1.1\"        \n [9,] \"http://www.statistics2013.org\"          \n[10,] \"http://drupal.org)\"                     \n[11,] \"http://gmpg.org/xfn/11\"                 \n[12,] \"http://ogp.me/ns\"                       \n[13,] \"http://www.rsscse.org.uk\"               \n[14,] \"http://purl.org/dc/terms\"               \n[15,] \"http://www.lib.ncsu.edu\"                \n[16,] \"http://www.flickr.com/photos/ncsu_scrc\" \n[17,] \"http://www.ncsu.edu/directory\"          \n[18,] \"http://mypack.ncsu.edu\"                 \n[19,] \"http://www.ncsu.edu/campus_map\"         \n[20,] \"http://schema.org/Library\"              \n\nExtension of this Analysis to the Entire Internet\nThe internet today contains some 43.5 Billion webpages. To solve the Page Rank problem with today‚Äôs computational power is really only possible using iterative methods that can make use of \"warm starts\", where solutions from previous days runs can be used as good starting values, significantly reducing the necessary number of iterations for convergence.\n\n\n\n",
    "preview": "posts/2021-01-22-the-page-rank-algorithm/the-page-rank-algorithm_files/figure-html5/MakePlot-1.png",
    "last_modified": "2021-01-22T14:22:24-08:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-21-ranking-of-baseball-teams-a-model-based-approach/",
    "title": "Ranking Baseball Teams - A Model Based Approach",
    "description": "In this post we work out the mathematics and implement two methods for fitting\nsimple unstructured Bradley-Terry models from scratch.",
    "author": [
      {
        "name": "Logan Lossing",
        "url": {}
      }
    ],
    "date": "2021-01-21",
    "categories": [
      "R",
      "python",
      "linear algebra",
      "calculus",
      "probability"
    ],
    "contents": "\n\nContents\nThe Model\nDeriving a Minorization-Maximization Algorithm for Maximizing \\(L(\\gamma)\\)\nImplementaion of the MM Algorithm\n\nReparameterization and Newton‚Äôs Scoring Method\nImplementation of Newton‚Äôs Method\n\nResults\n\nOriginally created in 2014\nIn this post, we wish to use Fisher‚Äôs Scoring Method and the MM algorithm to fit a Bradley-Terry model in an effort to rank the baseball teams of the American League. An interesting property of this model is its simplicity; only information about which teams have played each other in the regular season and, for each game played, noting which team won is used in the ranking.\nThe data used to illustrate the implementation of these algorithms in this post consists of the regular season match records of every team in the American League in the MLB for the 2013. For simplicity, games against National league opponents are ignored.\nThe Model\nConsider a league of p teams (p=15 for the our example), where each team has a strength parameter \\(\\gamma_i >0\\). Team \\(i\\) beats team \\(j\\) with probability \\(\\frac{\\gamma_i}{\\gamma_i + \\gamma_j}\\). Let \\(y_{ij}\\) represent the number of times that team \\(i\\) beats team \\(j\\), and for convenience we take \\(y_{ii} = 0\\), i = 1,2,...,p.¬†If we assume independence between games then the log-likehood of this model is:\n\\[L(\\gamma) = \\sum_{i,j}{y_{ij}\\left [ \\ln(\\gamma_i) - \\ln(\\gamma_i + \\gamma_j) \\right]}\\]\nIs the log-likelihood concave?\nTo determine the concavity of the log-likelihood function, we will take a look at the Hessian. Expanding the log-likelihood a little helps with taking the derivatives. Doing so we obtain,\n\\[L(\\gamma) = y_{12}(\\ln\\gamma_1 - \\ln(\\gamma_1 + \\gamma_2)) + y_{13}(\\ln\\gamma_2 - \\ln(\\gamma_1 + \\gamma_3)) + ... + y_{p(p-1)}(\\ln\\gamma_p - \\ln(\\gamma_{p} + \\gamma_{p-1}))\\]\nThus the gradient is given by:\n\\[\\nabla L(\\gamma)_k = \\left \\{ \\sum_{i=1}^p{y_{ik} \\left ( \\frac{1}{\\gamma_i} - \\frac{1}{\\gamma_i + \\gamma_k} \\right ) } - \\sum_{i=1}^p{y_{ki} \\left ( \\frac{1}{\\gamma_i + \\gamma_k} \\right )} \\right \\}, \\quad \\text{for} \\quad k = 1,2,...,p\\]\nAgain, careful differentiation yield the Hessian given by:\n\\[\\nabla^2L(\\gamma)_{lk} = \n\\begin{cases}\n\\sum_{j=1}^p{y_{lj}\\left [{ \\frac{-1}{\\gamma_l^2} + \\frac{1}{(\\gamma_l + \\gamma_j)^2}}\\right]} + \\sum_{j=1}^p{y_{jk}(\\frac{1}{(\\gamma_j + \\gamma_k)^2})}, & k = l \\\\\n(y_{lk} + y_{kl})\\frac{1}{(\\gamma_l + \\gamma_k)^2}, & k \\neq l\n\\end{cases}\\]\nNow, is the log-likelihood concave?\nNo, it does not appear to be so. To see this consider the scenario where one team is particularly bad and fails to win any games against any opponents in a given year. If this is the case, the Observed Information Matrix will have a negative on the diagonal and thus the likelihood cannot be concave.\nDeriving a Minorization-Maximization Algorithm for Maximizing \\(L(\\gamma)\\)\nFor this section, we want to find a minorizing function \\(g(\\gamma|\\gamma^{(t)})\\) that has the property that \\(g(\\gamma^{(t)}|\\gamma^{(t)}) = L(\\gamma^{(t)})\\) at \\(\\gamma = \\gamma^{(t)}\\) and otherwise, for all \\(\\gamma\\) we have that \\(g(\\gamma|\\gamma^{(t)}) < L(\\gamma)\\). These conditions are referred to as dominance and tangency conditions.\nThe supporting hyperplane inequality states that a differentiable function \\(f(\\cdot)\\) is convex in U \\(\\iff\\) \\(f(x) \\ge f(y) + \\langle \\nabla f(y), x-y \\rangle\\) for all \\(x,y \\in U\\).\nCondider the function \\(f(x) = -\\ln(x)\\) a convex function. Thus the supporting hyperplane inequality yields \\(-\\ln(x) \\ge -\\ln(y) + \\frac{-(x-y)}{x}\\). Letting \\(x = \\gamma_i + \\gamma_j\\) and \\(y = \\gamma_i^{(t)} + \\gamma_j^{(t)}\\) then we obtain:\n\\[\\begin{aligned}\n-\\ln(\\gamma_i + \\gamma_j) &\\ge -\\ln(\\gamma_i^{(t)} + \\gamma_j^{(t)}) + \\frac{-(\\gamma_i + \\gamma_j-\\gamma_i^{(t)} + \\gamma_j^{(t)})}{\\gamma_i^{(t)} + \\gamma_j^{(t)}} \\\\\n&= -\\frac{\\gamma_i + \\gamma_j}{\\gamma_i^{(t)} + \\gamma_j^{(t)})} + c(t) \\quad \\text{where c(t) is a constant with regard optimization}\\end{aligned}\\]\nNote, that the above inequality yield strict inequality when \\(\\gamma \\neq \\gamma^{(t)}\\) and equality precisely when \\(\\gamma = \\gamma^{(t)}\\).\nPlugging this expression into the log-likelihood from above we obtain an expression for the minorizing function that satisfies both properties listed above: dominance and tangency at \\(\\gamma^{(t)}\\). This minorizing function is given below.\n\\[\\begin{aligned}\ng(\\gamma|\\gamma^{(t)}) &= \\sum_{i,j}{y_{ij}\\left [ \\ln(\\gamma_i) -\\ln(\\gamma_i^{(t)} - \\gamma_j^{(t)}) + \\frac{-(\\gamma_i + \\gamma_j-\\gamma_i^{(t)} - \\gamma_j^{(t)})}{\\gamma_i^{(t)} + \\gamma_j^{(t)}} \\right]} \\\\\n&= \\sum_{i,j}{y_{ij}\\left [ \\ln(\\gamma_i)  + \\frac{-(\\gamma_i + \\gamma_j)}{\\gamma_i^{(t)} + \\gamma_j^{(t)}}  + c(t) \\right]} \\end{aligned}\\]\nThe benefit here is that the MM updates can be calculated analytically and thus computed with incredible efficiency.\nTo do this we first differentiate to obtain:\n\\[\\nabla g(\\gamma)_k = \\sum_{j=1}^p{y_{kj} \\left (\\frac{1}{\\gamma_k} - \\frac{1}{\\gamma_k^{(t)} + \\gamma_j^{(t)}} \\right )} - \\sum_{j=1}^p{y_{jk} \\left (\\frac{1}{\\gamma_k^{(t)} + \\gamma_j^{(t)}} \\right )}\\]\nSetting this equal to zero and solving for \\(\\gamma_k\\) then we have our updates.\n\\[\\gamma_k^{(t+1)} = \\frac{\\sum_{j=1}^p{y_{kj}}}{\\sum_{j=1}^P{\\left [ \\frac{y_{jk} + y_{kj}}{\\gamma_k^{(t)} + \\gamma_j^{(t)}}\\right ]}}\\]\nImplementaion of the MM Algorithm\nThe following is my code implementing the MM algorithm for these data. This algorithim is amazingly simple and efficient for these data. First it is necessary to read in the data and create a few functions to allow the main routine to be uncluttered. Also note, the parameters of this model are not identifiable without some remedy which was, in this case, to set \\(\\gamma_1 = 1\\).\nFirst, we need to get the data from ESPN, we can scrape the standings using the following Python routine:\n\nfrom bs4 import BeautifulSoup\nimport urllib2\nimport re\nimport csv\n\ncsv_filename = 'AL-standings.csv'\n\nyear = '2013'\nurl = 'http://espn.go.com/mlb/standings/grid/_/year/' + year\n\npage = urllib2.urlopen(url)\nsoup = BeautifulSoup(page.read())\n\n# Extracts the table for the American League (AL) and the rows for each team\nAL_table = soup.find(text = re.compile(\"American\")).find_parent(\"table\")\nAL_rows = AL_table.findAll('tr', class_ = re.compile(\"team\"))\n\n# Creates a list of the AL teams and then appends NL for National League\nAL_teams = [team_row.find('b').text for team_row in AL_rows]\nAL_teams.append(\"NL\")\n\n# Opens a CSV file for the AL standings\nwith open(csv_filename, 'wb') as f:\n    csv_out = csv.writer(f)\n    csv_out.writerow(['Team', 'Opponent', 'Wins', 'Losses'])\n    \n    # For each team in the AL table, identifies the team's name, the opponent,\n    # and their wins and losses (WL) against that opponent. Then outputs the\n    # results to the open CSV file\n    for team_row in AL_rows:\n        team = team_row.find('b').text\n        \n        # A cell has the following form:\n        # <td align=\"right\">\n        # 7-9<\/td>\n        WL_cells = team_row.findAll('td', align = \"right\")\n        \n        # Extracts the values for both wins and losses from each WL table cell\n        wins_losses = [td_cell.text.strip('\\n').split('-') for td_cell in WL_cells]\n        \n        # Writes the current team's standings to the CSV file\n        for i, opponent in enumerate(AL_teams):\n            if team != opponent:\n                csv_out.writerow([team, opponent, wins_losses[i][0], wins_losses[i][1]])\n\nAfter writing the data to csv, we then read the data back in with R to finish the analysis. In the code below, you can see we have square matrix with each row representing a team in the AL and each column representing each of the possible AL opponents.\nFor a fixed row, the number in a specific column corresponds to the number of games won vs the opponent listed with that column label. Example: In the first row, second column we see 11. This means that the BAL(timore) Orioles beat the BOS(ton) Red Sox 11 times during regular season play. The entry in row 2, column 1 is an 8, meaning that the BOS(ton) Red Sox beat the BAL(timore) Orioles 8 times during regular season play. No other data is used for this analysis. (The results are suprisingly good for such a simple model!)\n\n\n## Import Data\npath_to_data <- \"./ymat.csv\"\nymat <-read.csv(path_to_data, header = TRUE, as.is = TRUE)\nhead(ymat)\n\n\n    BAL BOS CHW CLE DET HOU KC LAA MIN NYY OAK SEA TB TEX TOR\nBAL   0  11   4   3   4   4  3   5   3   9   5   2  6   5  10\nBOS   8   0   4   6   3   6  2   3   4  13   3   6 12   2  11\nCHW   3   2   0   2   7   3  9   3   8   3   2   3  2   4   4\nCLE   4   1  17   0   4   6 10   4  13   1   5   5  2   5   4\nDET   2   4  12  15   0   6  9   0  11   3   3   5  3   3   5\nHOU   2   1   4   1   1   0  2  10   1   1   4   9  2   2   3\n\nymat <- apply(ymat,1,as.numeric)\nrownames(ymat) <- colnames(ymat)\nymat <- t(ymat)\n\n##############################################################################\n##  Calculating the likelihood\nsum.matrix.product.diagonals <- function(matA,matB){\n  ## Two nxn matrices, only forms the diagonal entries of the product\n  len <- dim(matA)[1]\n  return(sum(sapply(1:len,function(i) matA[i,]%*%matB[,i])))\n}\nloglikelihood.gamma <- function(gam,dat){\n  ##  Inputs:  parameter vector gam and data matrix dat\n  ##  Returns: scalar value of the log-likelihood\n  len <- length(gam)\n  entries <- outer(1:len,1:len,function(i,j) gam[i]/(gam[i]+gam[j]))\n  log.matrix <- log(entries)\n  return(sum.matrix.product.diagonals(log.matrix,t(dat)))\n}\n##############################################################################\n## Calculating the Gradient\ngradient.loglik <- function(gam,dat){\n  len <- length(gam)\n  mat1 <- outer(1:len,1:len,function(i,j) gam[i]/(gam[i]*(gam[i]+gam[j])))*dat\n  mat2 <- t(outer(1:len,1:len,function(i,j) 1/(gam[i]+gam[j])))*dat\n  gradient <- colSums(mat1) - colSums(mat2)\n  return(gradient)\n}\n##############################################################################\n## MM Update\nMM.compute.gam.update<- function(gam,dat){\n  ## Given the current value of the gam at time t, and the data matrix\n  ## Calcumates the the updated value for t+1, returns a vector\n  num <- rowSums(dat)\n  denom <- rowSums((dat+t(dat))/outer(gam,gam,\"+\" ))\n  updateRank <- num / denom\n  return(updateRank) \n}\n######################################################################\n##  Good Starting Values\ninitial.estimate <- function(dat){\n  ## Take the win% of all games played as a reasonable starting point\n  gamma.MM <- rowSums(dat)/(rowSums(dat) + colSums(dat))\n  gamma.MM <- gamma.MM/gamma.MM[1]\n  return(gamma.MM)\n}\n\n\n\nNow that all the setup functions have been loaded we can bring in the main routine that actually will return the estimated strengths for each team in the American League.\n\n\nstrength.fit.MM <- function(dat, gamma0 = NULL,maxiters = 1000,tolfun = 1e-6){\n\n  if(is.null(gamma0)){\n    gamma.t <- initial.estimate(dat)\n  } else{gamma.t <- gamma0}\n  \n  for(i in 1:maxiters){\n    \n    gamma.tp1 <- MM.compute.gam.update(gamma.t,dat) ##  Update\n    gamma.tp1 <- gamma.tp1/gamma.tp1[1] ## Normalize for identifiability\n    \n    loglik.tp1 <- loglikelihood.gamma(gamma.tp1,dat)  ## loglik at update\n    loglik.tp <- loglikelihood.gamma(gamma.t,dat) ## loglik at previous\n    improve.lik <- abs(loglik.tp1 - loglik.tp) ## Amount of improvement\n    \n    if(improve.lik<tolfun){ ## Have I converged?\n      gradient <- gradient.loglik(gamma.tp1,dat)\n      return(list(\"Strengths\" = gamma.tp1, \"Iterations\" = i,\n        \"Maximum\" = loglik.tp1,\"Gradient\" = gradient))\n    } else{ gamma.t <- gamma.tp1} ## If not, CONTINUE TO ITERATE\n    \n  } # end for\n  print(\"No Convergence\")\n  return(gamma.tp1)\n}\nranking <- strength.fit.MM(ymat)\nranking\n\n\n$Strengths\n      BAL       BOS       CHW       CLE       DET       HOU        KC \n1.0000000 1.2510850 0.5642156 1.1030067 1.1009506 0.3857875 0.9993401 \n      LAA       MIN       NYY       OAK       SEA        TB       TEX \n0.7532999 0.6160583 1.0492604 1.1049180 0.6650877 1.1384163 1.0401907 \n      TOR \n0.7518629 \n\n$Iterations\n[1] 18\n\n$Maximum\n[1] -711.5312\n\n$Gradient\n          BAL           BOS           CHW           CLE           DET \n 0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00 \n          HOU            KC           LAA           MIN           NYY \n 0.000000e+00  7.105427e-15 -7.105427e-15  0.000000e+00 -7.105427e-15 \n          OAK           SEA            TB           TEX           TOR \n 0.000000e+00  7.105427e-15  0.000000e+00  0.000000e+00  0.000000e+00 \n\nIn the results section, the results of the analysis will be described in detail. Now it is of interest to perform the exact same analysis using a Newton Algorithim.\nReparameterization and Newton‚Äôs Scoring Method\nFor this section, consider the reparameterization \\(\\gamma_i = e^{\\lambda_i}\\) for \\(\\lambda \\in \\mathbb{R}\\). The log-likelihood now becomes:\n\\[L(\\lambda) = \\sum_{i,j}{y_{ij}\\left [ \\lambda_i - \\ln(e^{\\lambda_i} + e^{\\lambda_i}) \\right]}\\]\nDifferentiating yields the following gradient:\n\\[\\nabla L(\\lambda)_k = \\sum_{j=1}^k{y_{kj} \\left ( 1 - \\frac{e^{\\lambda_k}}{e^{\\lambda_k + \\lambda_j}} \\right )} + \\sum_{j=1}^p{y_{jk}\\left ( \\frac{-e^{\\lambda_k}}{e^{\\lambda_k} + e^{\\lambda_j} }\\right)}\\]\nThe Hessian is given by:\n\\[\\nabla^2 L(\\lambda)_{kj} = \n\\begin{cases}\n-\\left [  \\sum_{j=1}^p{(y_{kj} + y_{jk})\\left ( \\frac{e^{\\lambda_k + \\lambda_j}}{ (e^{\\lambda_j} + e^{\\lambda_k})^2} \\right )} \\right ], & k = j \\\\\n\\frac{(y_{kj} + y_{jk})e^{\\lambda_j + \\lambda_k}}{(e^{\\lambda_j} + e^{\\lambda_k})^2}, & k \\neq j\n\\end{cases}\\]\nFor this reparameterized likelihood, the resulting Hessian matrix is negative definite and thus the log-likelihood under this representation is concave. This result is due to the closure of log-convex functions under addition (see \"Convex Optimization\"-Stephen Boyd pg.105) Further, since we have reparameterized our parameters to the real line, there is no need to worry about backtracking in the implementation of Newton‚Äôs Method.\nIt is now time to obtain the estimates of our parameters using Newton‚Äôs Method. The implementation of this method is below.\nImplementation of Newton‚Äôs Method\nThis method has quite a few more \"moving parts\" than did the MM implementation. Not only does the likelihood need to be computed, but also the gradient and Hessian matrices. The set-up for the actual main routine is below.\n\n\nloglikelihood.lambda <- function(lam,dat){\n  len <- length(lam)\n  entries <- outer(1:len,1:len,function(i,j) lam[i]-log(exp(lam[i])+exp(lam[j])) )\n  log.lik <- sum(dat*entries)\n  return(log.lik)\n}\n##############################################################################\n## Good starting point\ninitial.estimate.lambda <- function(dat){\n  lambda.Newt <- log(rowSums(dat)/(rowSums(dat)+colSums(dat)))\n  lambda.Newt <- lambda.Newt - lambda.Newt[1]\n  return(lambda.Newt) \n}\n###############################################################################\n## Gradient\ngradient.loglik.lam <- function(lam,dat){\n  len <- length(lam)\n  mat1 <- outer(1:len,1:len,function(i,j) exp(lam[i])/(exp(lam[i]) + exp(lam[j])))\n  mat1 <- mat1*(dat + t(dat))\n  gradient <- rowSums(dat) - rowSums(mat1)\n  return(gradient)\n}\n###############################################################################\n## Observed Fisher Information Matrix\nobs.info.lam <- function(lam,dat){\n  len <- length(lam)\n  mat1 <- outer(1:len,1:len,\n    function(i,j) exp(lam[i]+lam[j])/(exp(lam[i])+exp(lam[j]))^2)*(dat + t(dat))\n  diagonals <- rowSums(mat1)\n  mat1 <- -1*mat1\n  diag(mat1) <- diagonals\n  return(mat1)\n}\n#############################################################################\n## Modified Cholesky Decomposition, creates positive definite approximation\nmodifiedCholeskyUpdate <- function(matrixA,matrixL.km1){\n  ############################################################\n  ##  Inputs:  matrixA- is the KxK matrix we are decomposing\n  ##           matrixL.km1- the K-1xK-1 lower triangular matrix\n  ##                        obtained from previous iterations\n  ##  Outputs: matrixL.k- the KxK lower tri matrix\n  #############################################################\n  \n  dimension <- dim(matrixA)[1]\n  \n  a.k.vector <- matrixA[-dimension,dimension,drop=F]\n  a.kk <- matrixA[dimension,dimension,drop=F]\n  \n  l.k.vector <- forwardsolve(matrixL.km1,a.k.vector,k=dimension-1)\n  check <- a.kk - t(l.k.vector)%*%l.k.vector\n  if(check >0){\n    l.kk <- sqrt(a.kk - t(l.k.vector)%*%l.k.vector)\n  }else{l.kk <- .01}\n  \n  matrixL.k <-cbind(matrixL.km1,rep(0,dimension-1))\n  last.row <- c(l.k.vector,l.kk)\n  matrixL.k <- rbind(matrixL.k,last.row)\n  matrixL.k <- as.matrix(matrixL.k)\n  rownames(matrixL.k) <- NULL\n  colnames(matrixL.k) <- NULL\n  return(matrixL.k)\n}\nmodifiedCholeskyDecomp <-function(matrixA){\n  ##################################################################\n  ##  Inputs:  matrixA- is the nxn matrix we are decomposing     \n  ##  Outputs: L- the nxn lower tri matrix such that matrixA = LL^t\n  ##################################################################\n  \n  dimension <- dim(matrixA)\n  if(dimension[1] != dimension[2]){return(Warning = \"Non-Square Matrix\")}\n  if(!isSymmetric(matrixA)){return(Warning = \"NOT SYMMETRIC\")}\n  \n  A2 <- matrixA[1:2,1:2,drop=F]\n  \n  if(as.numeric(matrixA[1,1]) >= 0){\n    L1 <- sqrt(matrixA[1,1,drop=F])\n  }else{ L1 <- .01}\n  \n  for(i in 1:(dimension[1]-1)){\n    L1 <- modifiedCholeskyUpdate(A2,L1)\n    if(i<dimension[1]-1){\n      A2 <- matrixA[1:(i+2),1:(i+2),drop = F]\n    }\n  }\n  return(L = L1)\n}\n\n\n\nNow that all the necessary functions have been created, we can call the main routine and obtain some estimates. This implementation is below.\n\n\nstrength.fit.Newton <- function(dat,lambda0=NULL,maxiters=100,tolfun=1e-6){\n  \n  if(is.null(lambda0)){\n    lambda.t <- initial.estimate.lambda(dat)\n  } else{lambda.t <- lambda0}\n  len <- length(lambda.t)\n  \n  for(i in 1:maxiters){\n    \n    gradient.t <- gradient.loglik.lam(lambda.t,dat) \n    lower <- modifiedCholeskyDecomp(obs.info.lam(lambda.t,dat)) ## Find dir to ascend\n    l.inverse.times.gradient <- forwardsolve(lower,gradient.t)\n    x <- backsolve(t(lower),l.inverse.times.gradient)\n    \n    lambda.tp1 <- lambda.t + x  ## Get update\n    lambda.tp1 <- lambda.tp1-lambda.tp1[1]  ## Normalize\n    \n    loglik.tp1 <- loglikelihood.lambda(lambda.tp1,dat)  ## loglik at update\n    loglik.tp <- loglikelihood.lambda(lambda.t,dat) ## loglik at previous\n    improve.lik <- abs(loglik.tp1 - loglik.tp)  ## Improvement in likelihood\n    \n    if(improve.lik<tolfun){  ## Have we CONVERGED?\n      gradient <- gradient.loglik.lam(lambda.tp1,dat)\n      return(list(\"Strengths\" = lambda.tp1, \"Iterations\" = i,\"Maximum\" = loglik.tp1,\n              \"Gradient\" = gradient))\n    } else{ lambda.t <- lambda.tp1}\n    \n  } # end for\n  print(\"No Convergence\")\n  return(lambda.tp1)\n}\nstrengths <- strength.fit.Newton(ymat)\nstrengths\n\n\n$Strengths\n          BAL           BOS           CHW           CLE           DET \n 0.0000000000  0.2240370238 -0.5724141333  0.0979381904  0.0960691213 \n          HOU            KC           LAA           MIN           NYY \n-0.9525885657 -0.0007575662 -0.2834240107 -0.4845092637  0.0480898408 \n          OAK           SEA            TB           TEX           TOR \n 0.0996121698 -0.4079668384  0.1296495476  0.0392569072 -0.2852165434 \n\n$Iterations\n[1] 3\n\n$Maximum\n[1] -711.5312\n\n$Gradient\n          BAL           BOS           CHW           CLE           DET \n 1.697572e-09  2.494630e-09 -3.477396e-11  2.190689e-09  2.179974e-09 \n          HOU            KC           LAA           MIN           NYY \n-3.202133e-08  1.669122e-09  3.090079e-09  4.267733e-10  1.770061e-09 \n          OAK           SEA            TB           TEX           TOR \n 5.669420e-09  2.072611e-09  2.230038e-09  5.205393e-09  1.359737e-09 \n\nLook! We have obtained the same results as before and in very few iterations, the strength of this method.\nWhat Structure of the Hessian Could be Exploited When P is Very Large?\nIf this method was to be applied to a very large tournament, then it would be likely that most competitors would only play a tiny fraction of the possible opponents. This would result in a very sparse Hessian matrix whose structure could be exploited using Newton‚Äôs Method.\n\nResults\nNow that we have investigated how to obtain estimates for the strength parameters in our model, what do they tell us about the American League in MLB baseball?\nTeam\nStrength\nWin%\nTotal Wins\nBOS\n1.25\n0.58\n83.00\nTB\n1.14\n0.56\n80.00\nOAK\n1.10\n0.58\n83.00\nCLE\n1.10\n0.57\n81.00\nDET\n1.10\n0.57\n81.00\nNYY\n1.05\n0.54\n76.00\nTEX\n1.04\n0.57\n81.00\nBAL\n1.00\n0.52\n74.00\nKC\n1.00\n0.54\n77.00\nLAA\n0.75\n0.48\n68.00\nTOR\n0.75\n0.44\n63.00\nSEA\n0.67\n0.44\n63.00\nMIN\n0.62\n0.41\n58.00\nCHW\n0.56\n0.39\n55.00\nHOU\n0.39\n0.30\n43.00\nFirst consider the table above. The ‚ÄúStrength\" column shows us, from best to worst, the rankings provided by this model via estimation of the strength parameters. The‚ÄùWin%\" column shows each teams actual percentage of games won. It is interesting to note the differences in ranking and actual winning%.\nThis discrepancy appears to be related to certain divisions in the American League having more strong teams than others. Since teams within a division play more, this model adjusts for playing better opponents. It is also interesting to note that the Boston Red Sox are ranked number one in this model and would actually go on to win the World Series in 2013.\nSince we have a way to turn our strength parameter estimates into probailities, the table below shows the probability any team in the AL would win in a match against any other opponent. The way to read the table is to pick a row, and then the numbers in that row are the probability that the team indicated by the row would beat the team in the column.\n\nBAL\nBOS\nCHW\nCLE\nDET\nHOU\nKC\nLAA\nMIN\nNYY\nOAK\nSEA\nTB\nTEX\nTOR\n\nBAL\n\n0.44\n0.64\n0.48\n0.48\n0.72\n0.50\n0.57\n0.62\n0.49\n0.47\n0.60\n0.47\n0.49\n0.57\n\nBOS\n0.56\n\n0.69\n0.53\n0.53\n0.76\n0.56\n0.62\n0.67\n0.54\n0.53\n0.65\n0.52\n0.55\n0.62\n\nCHW\n0.36\n0.31\n\n0.34\n0.34\n0.59\n0.36\n0.43\n0.48\n0.35\n0.34\n0.46\n0.33\n0.35\n0.43\n\nCLE\n0.52\n0.47\n0.66\n\n0.50\n0.74\n0.53\n0.59\n0.64\n0.51\n0.50\n0.62\n0.49\n0.52\n0.59\n\nDET\n0.52\n0.47\n0.66\n0.50\n\n0.74\n0.52\n0.59\n0.64\n0.51\n0.50\n0.62\n0.49\n0.51\n0.59\n\nHOU\n0.28\n0.24\n0.41\n0.26\n0.26\n\n0.28\n0.34\n0.39\n0.27\n0.26\n0.37\n0.25\n0.27\n0.34\n\nKC\n0.50\n0.44\n0.64\n0.47\n0.48\n0.72\n\n0.57\n0.62\n0.49\n0.47\n0.60\n0.47\n0.49\n0.57\n\nLAA\n0.43\n0.38\n0.57\n0.41\n0.41\n0.66\n0.43\n\n0.55\n0.42\n0.41\n0.53\n0.40\n0.42\n0.50\n\nMIN\n0.38\n0.33\n0.52\n0.36\n0.36\n0.61\n0.38\n0.45\n\n0.37\n0.36\n0.48\n0.35\n0.37\n0.45\n\nNYY\n0.51\n0.46\n0.65\n0.49\n0.49\n0.73\n0.51\n0.58\n0.63\n\n0.49\n0.61\n0.48\n0.50\n0.58\n\nOAK\n0.53\n0.47\n0.66\n0.50\n0.50\n0.74\n0.53\n0.59\n0.64\n0.51\n\n0.62\n0.49\n0.52\n0.59\n\nSEA\n0.40\n0.35\n0.54\n0.38\n0.38\n0.63\n0.40\n0.47\n0.52\n0.39\n0.38\n\n0.37\n0.39\n0.47\n\nTB\n0.53\n0.48\n0.67\n0.51\n0.51\n0.75\n0.53\n0.60\n0.65\n0.52\n0.51\n0.63\n\n0.52\n0.60\n\nTEX\n0.51\n0.45\n0.65\n0.48\n0.49\n0.73\n0.51\n0.58\n0.63\n0.50\n0.48\n0.61\n0.48\n\n0.58\n\nTOR\n0.43\n0.38\n0.57\n0.41\n0.41\n0.66\n0.43\n0.50\n0.55\n0.42\n0.41\n0.53\n0.40\n0.42\n\n\nFinally, it is always nice to visualize the above table so that we can easily see which teams are good and which teams are not so good. A heat map version of the above table can do just this. The plot below conveys the exact same information but rather than being interested in specific values, we are more interested in the patterns, easily pointing out which teams have performed poorly in the 2013 regular season. For example, the lowest ranked team, the Houston Astros, have a red band all the way across their row, showing a low probability of beating any opponent whereas the Boston Red Sox‚Äôs row is mostly blue and purple, showing their dominance over the rest of the league.\nHeat Map of the Estimated Win Probabilities for Teams in the American League\n\n\n",
    "preview": "posts/2021-01-21-ranking-of-baseball-teams-a-model-based-approach/heatMap.png",
    "last_modified": "2021-01-22T14:23:29-08:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-20-welcome/",
    "title": "Welcome",
    "description": "The journey begins...",
    "author": [
      {
        "name": "Logan Lossing",
        "url": {}
      }
    ],
    "date": "2021-01-20",
    "categories": [],
    "contents": "\nTechnology is amazing. Modern frameworks and computing architectures are more powerful than ever. Despite the progress, it can be quite time consuming to learn a new technology, especially if not done in an organized way. To be a little more thoughtful and organized with my own professional development, I decided to start this blog where I can turn my random musings into more polished articles for the benefit of myself and anyone else that finds it useful.\nShout out to all of the amazing developers around the world that share their amazing materials with the world! You all are an inspiration.\nLink to talk by David Robinson - The Unreasonalbe Effectiveness of Public Work\n\n\n\n",
    "preview": {},
    "last_modified": "2021-01-22T14:19:40-08:00",
    "input_file": {}
  }
]
