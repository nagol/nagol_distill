[
  {
    "path": "posts/2021-01-30-getting-things-done-notes/",
    "title": "Getting Things Done - Notes",
    "description": "Getting Things Done - the Art of Stree-Free Productivity is a New York Times\nbestseller and has been recommended to me personally many times over the years.\nNow, more than ever before, I find myself desiring a better, more principled\nmethod of staying organized and \nkeeping my priorities straight...something that the book claims to be able to help\nwith. This post is just a collection of the important points that I found to be\nuseful. Your mileage may vary.",
    "author": [
      {
        "name": "Logan Lossing",
        "url": {}
      }
    ],
    "date": "2021-01-30",
    "categories": [
      "productivity",
      "book-review"
    ],
    "contents": "\n\nContents\nChapter 1\n\nWork in progress…\nChapter 1\nThe introduction is catchy; most people strive to increase efficiency in their lives and leave few loose ends. I think many will see themselves described in the opening paragraphs…life can often have too much to juggle when you don’t have a clear, developed working theory of juggling.\nThe book claims that the system presented will accomplish 3 Main Objectives:\nCapture all the things that might need to get done or have usefulness for you - now, later, someday, big, little, or in between - in a logical and trusted system outside your head and off your mind\nDirecting yourself to make front-end decisions about all of the ‘inputs’ you let into your life so that you always will have a workable inventory of ‘next actions’ that you can implement or renegotiate in the moment\nCurating and coordinating all of that content, utilizing the recognition of the multiple levels of commitments with yourself and others you will have at play, at any point in time.\nI find these overall goals logical and simple…we need to have a budget for our time and attention as we do for our finances. We only have a fixed number of hours in the day to get things done. Without noticing (and even partially caused by not tracking) the myriad of little duties and commitments we make eat up all our precious time.\nThe first chapter mentions the relief that can be enjoyed by ‘offloading’ all these little tasks in our minds for an appropriate time + place to make actual progress and not just stressy-panic. Further, having a predefined course of action for how to optimally handle situations that might arise reminds of stoic ideas of visualizing our fears, mentally stepping through what can happen and how we might act in response.\nThe whole time reading through the first chapter, I daydreamed about a super AI assistant that I could trust to help manage situations as they arise. Learn from experience, customize its behavior in response to my needs, and seek to optimize my stress levels and productivity as measured by some biometic feedback and assessment of the usefulness of the work performed.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-02-02T18:48:38-08:00",
    "input_file": "getting-things-done-notes.utf8.md"
  },
  {
    "path": "posts/2021-01-30-sql-basics-from-the-terminal/",
    "title": "SQL Basics from the Terminal",
    "description": "Learn how to create a SQL database quickly from the terminal. Useful for\nbeginners who want a no nonsense bare bones method of playing with SQL databases\nor for anyone looking to create minimal examples for \ntesting and development.",
    "author": [
      {
        "name": "Logan Lossing",
        "url": {}
      }
    ],
    "date": "2021-01-30",
    "categories": [
      "SQL",
      "terminal",
      "databases"
    ],
    "contents": "\n\nContents\nWhy learn SQL from the terminal?\nInstall SQLite\nCRUD\nCreate Tables\nThinking About Table Design - Schema\nAdding Data to the Database Tables\n\nRead\nUpdate\nDelete\nTables\nDeleting Rows in Tables\n\nWrap Up\n\n\nWhy learn SQL from the terminal?\nWithout an IDE to worry about or any real configuration details you can focus on just SQL basics…with none of the headaches. Personally, I often create small examples tables when designing a new database or trying to design a complex query.\nFor this quick tutorial, we will be using SQLlite. There are many flavors of SQL and this one is the quickest and easiest to install.\nInstall SQLite\nThe SQLite.org contains instructions for installing on your operating system but for macOS, you can quickly install SQLite using homebrew brew install sqlite. Once installed, verify that SQLite is installed properly using sqlite3 --version.\nCRUD\nThe core SQL database operations are known by the acronym C.R.U.D. - (C)reate, (R)ead, (U)pdate, and (D)elete so that should be a good first set of thing to learn about.\nIf you are not coming from a strong database background and are more comfortable with R or python, you may first want to soon get a feel for how a table in a database is different from a tibble/data_frame. The good news is, in practice, much more is similar than is different. However, databases offer an entire array of new possibilities and control…enough to know in fact that managing databases can be an entire job.\nCreate Tables\nNow that we have SQLite installed, we probably want to first create some tables.\nTo create a database containing a table, we need only provide names for the columns of the database and provide SQL with information about the type of data for each column. The SQLite documentation contains extensive information about all the allowable type for SQLite but the main ones to get started with are:\nNULL\nInteger\nReal\nText\nTo write a SQL scrpt, we need a text editor we can use to write our code. Any simple code editor will do (atom, sublime, vscode,…). Go ahead an open up a blank document in your text editor so it will be ready to go when we are ready to start writing some code.\nGo ahead and open your terminal and create a directory somewhere convenient to store our example SQLite database and .sql scripts.\ncd ~/Desktop\nmkdir sqlite_exploration\ncd sqlite_exploration\nWe should now be ready to create a database!\nThinking About Table Design - Schema\nBut what do we want to create?\nOften I create small example tables that resemble data that I need to work with but simplified. This way I can think through how to properly store the data so that it is easy to update and report off of.\nFor example, in a past life I was a Lecturer at a university. If I wanted to use SQL to manage and report on student grades, I might want to create a few tables to help me manage the course data efficiently and set me up for efficient reporting later.\neliminate duplication - We don’t want to see the same information repeated over and over in our tables. If an update is required, we want to update a single table if possible.\noperational efficiency - We should be able to easily query the data. Clear and documented relationships between tables greatly simplifies the cognitive overhead of keeping track of the data you are working with\nWith course grades as an example (I know, a little dull but at least most have experience with courses and grading), I might create three tables to split the data I need to keep track of into separate logical entities that have a clear relationship.\nHere is one possible simple schema for a gradebook:\nSTUDENTS - one row per student containing all student specific information:\nstudent_id - Integer Primary Key\nstudent_name - Text\nstudent_level (Freshmen, Sophmore, Junior, Senior) - Text\nmajor - Text\nemail - Text\nASSIGNMENTS - one row per assignment containing all assignment specific information:\nassignment_id - Integer Primary Key\nassignment_category - Text\nassignment_value - Real or Integer\nGRADE - one row per graded assignment per student:\nassignment_id - Integer\nstudent_id - Integer\nnumeric_grade - Integer or Real\nFor the STUDENTS and ASSIGNMENTS tables, we have a primary key (unique, non-null identifier) that we can use to reference a specific student or assignment. The rest of the fields in these two tables contain all the specific details about these entities. If something needs to be updated, we are going to update it in one table.\nThe final table, GRADE, is a relation table telling us how to join information from the STUDENTS and ASSIGNMENTS tables. For example, we might want the email addresses of all students that scored below 75% on a particular assignment. Using the GRADE table, SQL will be able to match rows using the student_id and assignment_id fields.\nIn SQLite, we use CREATE TABLE tbl_name (); with a comma separated list of columns to add to our table along with information about type in the form (name1 TYPE1, name2 TYPE2...).\nGo ahead and type the following (or just copy it) into your text editor. Save the file with the name table_schema.sql.\n\nCREATE TABLE students (\n    student_id INTEGER PRIMARY KEY,\n    name TEXT,\n    student_level TEXT,\n    major TEXT,\n    email TEXT\n);\n\nCREATE TABLE assignments (\n  assignment_id INTEGER PRIMARY KEY,\n  assignment_category TEXT,\n  assignment_value INTEGER\n);\n\nCREATE TABLE grade (\n  assignment_id INTEGER,\n  student_id INTEGER,\n  numeric_grade REAL\n  \n)\n\nTo create a database with tables according to our schema, just type $sqlite3 class.db < table_schema.sql. Use ls to see that a new class.db file has been created in your working directory.\nTo check everything is working, type sqlite class.db and a SQL prompt will appear. We don’t have any data in our tables yet, but you can type .schema to see a listing of the tables we have just created.\n\n(base) ➜  sql sqlite3 course.db                           \nSQLite version 3.28.0 2019-04-15 14:49:49\nEnter \".help\" for usage hints.\nsqlite> .schema\nCREATE TABLE students (\n    student_id INTEGER PRIMARY KEY,\n    name TEXT,\n    student_level TEXT,\n    major TEXT,\n    email TEXT\n);\nCREATE TABLE assignments (\n  assignment_id INTEGER PRIMARY KEY,\n  assignment_category TEXT,\n  assignment_value INTEGER\n);\nCREATE TABLE grade (\n  assignment_id INTEGER,\n  student_id INTEGER,\n  numeric_grade REAL\n  \n);\n\nType .quit to exit the SQL prompt and get back to the terminal.\nAdding Data to the Database Tables\nLet’s add some example data to our tables to play with. We are going to do it the old fashioned way, insert rows into our tables…manually. We are keeping it simple here, we can always add complexity later.\nOpen another blank document and name this file class_data.sql.\nThe simplest way for us to add data to our table is with INSERT INTO tbl_name VALUES (val1, val2, ...). Notice in the code below, it is not required to list the column names in parentheses, but it can make it easier to remember what goes where.\nYou can just copy this piece into your script. For fun, add yourself to the student table, add a new assignment to the assignments table and assign some random grades for your new made up assignment.\nMake sure to remember that primary keys must be unique, SQL will throw an error if you try to add another observation with the same primary key.\n\n/* STUDENTS */\nINSERT INTO students (student_id, name, student_level, major, email)\n  VALUES (1, 'Student 1', 'Freshman', 'CS', 'student1@uni.edu');\n\nINSERT INTO students\n  VALUES (2, 'Student 2', 'Freshman', 'CS', 'student2@uni.edu');\n  \nINSERT INTO students \n  VALUES (3, 'Student 3', 'Freshman', 'MATH', 'student3@uni.edu');\n  \n  \n/* ASSIGNMENTS */\n    \nINSERT INTO assignments (assignment_id, assignment_category, assignment_value)\n  VALUES (1, 'quiz', 10);\n  \nINSERT INTO assignments \n  VALUES (2, 'quiz', 10);\n  \nINSERT INTO assignments\n  VALUES (3, 'exam', 100);\n  \n/* GRADES */\n    \nINSERT INTO grades (assignment_id, student_id, numeric_grade)\n  VALUES (1, 1, 0.85);\n  \nINSERT INTO grades\n  VALUES (2, 1, 0.75);\n  \nINSERT INTO grades\n  VALUES (3, 1, 0.9);\n  \nINSERT INTO grades \n  VALUES (1, 2, 0.7);\n  \nINSERT INTO grades\n  VALUES (2, 2, 0.6);\n  \nINSERT INTO grades\n  VALUES (3, 2, 0.65);\n  \nINSERT INTO grades \n  VALUES (1, 3, 0.99);\n  \nINSERT INTO grades\n  VALUES (2, 3, 1);\n  \nINSERT INTO grades\n  VALUES (3, 3, .92);\n\nBack at the terminal, type sqlite3 -echo class.db < class_data.sql to run the script class_data.sql on class.db. Now, the tables have some data and we can start playing with SQL code.\nRead\nTo execute a simple query we can create a new script called query.sql.\nThe simplest type of query we might want to run, might look like the following:\n\nSELECT (variables to select)\n  FROM (table name to select from)\n  WHERE (filtering conditions)\n\nFor example, add the following to your query.sql script.\n\nSELECT *\n  FROM students\n  WHERE major = 'CS'\n\nTo run the script, type sqlite3 -echo -column -header course.db < query.sql. The extra flags -column -header make the output much more readable.\n\n$sqlite3 -echo -column -header course.db < scripts/query.sql\n\nSELECT *\nFROM students\nWHERE major = 'CS'\nstudent_id  name        student_level  major       email           \n----------  ----------  -------------  ----------  ----------------\n1           Student 1   Freshman       CS          student1@uni.edu\n2           Student 2   Freshman       CS          student2@uni.edu\n\nAt this point, we are ready to start experimenting. For example, we could join all the tables together and select just the resulting columns of interest.\n\nSELECT students.student_id,\n       students.name,\n       assignments.assignment_category,\n       assignments.assignment_value,\n       grades.numeric_grade\n  FROM students, assignments, grades\n  ON students.student_id = grades.student_id\n    AND assignments.assignment_id = grades.assignment_id;\nstudent_id  name        assignment_category  assignment_value  numeric_grade\n----------  ----------  -------------------  ----------------  -------------\n1           Student 1   quiz                 10                0.85         \n1           Student 1   quiz                 10                0.75         \n1           Student 1   exam                 100               0.9          \n2           Student 2   quiz                 10                0.7          \n2           Student 2   quiz                 10                0.6          \n2           Student 2   exam                 100               0.65         \n3           Student 3   quiz                 10                0.99         \n3           Student 3   quiz                 10                1.0          \n3           Student 3   exam                 100               0.92  \n\nThere are plenty of books, blogs, and websites to learn all about SQL syntax. One thing that I find is missing at times is that hands on practice with tables that you created, answering some questions that you might already know how to find using other means.\nRecommended topics to explore:\nthe main sql verbs SELECT, WHERE, GROUP BY, HAVING, ORDER BY\njoins: INNER JOIN, LEFT JOIN\naggregation functions: AVG(), COUNT(), SUM()\nUpdate\nUpdating tables is less common in data analysis and more common as part of a data pipeline, application, or other database process. Updates are typically not made willy-nilly, permanent changes to the data are made.\nTo UPDATE a value in a table, use the synatax\nUPDATE tbl_name SET var_to_change WHERE filter_for_where_to_change\nHere you can just create another new script update.sql and copy in the following:\n\nUPDATE grades \n    set numeric_grade = 1.0 \n    where student_id = 3 \n        and assignment_id = 3;\n\nThen run sqlite3 course.db < scripts/update.sql\nand finally rerun our previous query to verify the values really did change\nsqlite3 -echo -column -header course.db < query.sql\n\nstudent_id  name        assignment_category  assignment_value  numeric_grade\n----------  ----------  -------------------  ----------------  -------------\n1           Student 1   quiz                 10                0.85         \n1           Student 1   quiz                 10                0.75         \n1           Student 1   exam                 100               0.9          \n2           Student 2   quiz                 10                0.7          \n2           Student 2   quiz                 10                0.6          \n2           Student 2   exam                 100               0.65         \n3           Student 3   quiz                 10                0.99         \n3           Student 3   quiz                 10                1.0          \n3           Student 3   exam                 100               1.0    \n\nOne dimension of richness of databases is the data integrity and data lineage tooling that they provide. Here we are making simple updates but more generally, in a business context, having precise control over what changes are made to the data and an ability to track those changes is of utmost importance.\nDelete\nTables\nWe can delete entire tables using DROP TABLE tbl_name but be careful, there is no recovery, If you have your table schema script separated like we did here, we can recreate our database quickly.\nLet’s go ahead and add the following snippet at the top of our table_schema.sql script.\n\nDROP TABLE students;\nDROP TABLE assignments;\nDROP TABLE grades;\n\nAside from demonstrating how to delete a table, this will allow us run the script more than once without having to manually rm course.db our database. Databases have built in mechanisms to prevent tables from being overwritten which can be a pain when you are just playing around.\nDeleting Rows in Tables\nDeleting an observation (or set of observations) is very similar to UPDATE; we supply SQL with a WHERE condition helping us to identify which rows to remove.\nAdd the following code to your update.sql script.and rerun the script.\n\nDELETE FROM assignments WHERE assignment_id = 3;\n\nYou should now see all the rows corresponding to assignment_id = 3 have been removed.\n\nstudent_id  name        assignment_category  assignment_value  numeric_grade\n----------  ----------  -------------------  ----------------  -------------\n1           Student 1   quiz                 10                0.85         \n1           Student 1   quiz                 10                0.75         \n2           Student 2   quiz                 10                0.7          \n2           Student 2   quiz                 10                0.6          \n3           Student 3   quiz                 10                0.99         \n3           Student 3   quiz                 10                1.0  \n\nWrap Up\nIn a few short minutes, we were able to get SQLite3 up and running in the terminal, we created a small example database and created small sql scripts to investigate basic SQL CRUD operations. Of course there is much more to learn, but you now have the tools to play around with schema design and query writing…all straight from the terminal.\nIn a future post, we will continue on with writing some more involved SQL report writing techniques and incorporate proper databases into our R, python, or SAS projects.\n\n\n\n",
    "preview": "posts/2021-01-30-sql-basics-from-the-terminal/sql_terminal.png",
    "last_modified": "2021-02-02T18:46:39-08:00",
    "input_file": "sql-basics-from-the-terminal.utf8.md"
  },
  {
    "path": "posts/2021-01-22-the-page-rank-algorithm/",
    "title": "The Page Rank Algorithm",
    "description": "Google's Page Rank Algorithm was an early tool that helped determine the\nrelevance of search results.\n\nHow does the algorithm work? How can we implement the algorithm?\n\nIn this post, we investigate the mathematics behind the Page Rank algorithm and\nimplement several different methods for obtaining results using R.",
    "author": [
      {
        "name": "Logan Lossing",
        "url": {}
      }
    ],
    "date": "2021-01-22",
    "categories": [
      "R",
      "linear algebra",
      "probability"
    ],
    "contents": "\n\nContents\nThe Page Rank Problem: An Exploration\nSummary Statistics of the NCSU Data\n\nSolving the Page Rank Problem\nUsing a Dense Linear System Solver\nUsing a Simple Iterative Linear System Solver\nUsing a Dense Eigen-Solver\nUsing a Simple Iterative Eigen-Solver\n\nThe Results\nExtension of this Analysis to the Entire Internet\n\n\nOriginally published in 2014\nThe Page Rank Problem: An Exploration\nWhen searching for a particular keyword on the internet, we want our search engine to return \"relevant\" websites. There are many possible ways of defining what we mean by relevant, but the one that the Google founders came up with in the 1990’s was the Page Rank method.\nThe crux of the idea of the Page Rank method is that the \"importance\" of a particular web-page can be defined to be the number of web-pages that link to it. The authors at http://www.math.cornell.edu/~mec/Winter2009/RalucaRemus/Lecture3/lecture3.html did a great job in explaining what this means, and I borrow their explanation here.\n\n\"If we create a web page i and include a hyperlink to the web page j, this means that we consider j important and relevant for our topic. If there are a lot of pages that link to j, this means that the common belief is that page j is important. If on the other hand, j has only one backlink, but that comes from an authoritative site k, (like www.google.com, www.cnn.com, www.cornell.edu) we say that k transfers its authority to j; in other words, k asserts that j is important. Whether we talk about popularity or authority, we can iteratively assign a rank to each web page, based on the ranks of the pages that point to it.\"\n- Cornell Math Explorers Club Website\n\nUsing this idea of importance, we can turn a directed graph, one that represents the network structure of webpages (nodes are websites, edges are links), into a transition matrix for a Markov Chain. Then we can think of the most \"relevant\" websites as those with the highest probabilities in the stationary distribution of the Markov Chain.\nThere are, however, two issues that we need to adjust for. The first is that our graph may not be fully connected. That is, we are not guaranteed that a path exists to every website from every other website. This causes us to get stuck in one part of the graph and not be able to assess the importance of other parts of the graph. The second issue is “dangling nodes,\" websites that have links to them but they do not link anywhere else. In order to adjust for this, we have the idea of a”random surfer.\"\nA \"random surfer\" is an entity that will randomly surf around the graph according to some basic rules. Let \\(r_{i}\\) be the out degree of webpage i. The out degree of a webpages is defined to be the number of webpages that it links to. When arriving to webpage i then they do one of the following things:\nIf \\(r_{i} > 0\\) then with probability p they uniformly chose a link on that page and with probability 1-p they choose a link uniformly from the space of n-pages\nIf \\(r_{i} = 0\\) then they choose a random page uniformly from the space of n-pages\nWe can notate this as follows.\nLet A be an (nxn) adjacency matrix where \\(a_{ij} = \\mathbb{1}(\\text{webpage i links to webpage j})\\)\nDefine p to be the probability that a random surfer follows a link on the current page (1-p is called the \"teleportation\" parameter)\n\\(r_{i}\\) be the out degree of webpage i. The out degree of a webpages is defined to be the number of webpages that it links to\nThe transition matrix P has entries \\(p_{ij} = \\mathbb{1}(r_{i}>0)\\Big (\\frac{pa_{ij}}{r_{i}} \\Big ) + \\mathbb{1}(r_{i}=0) \\Big ( \\frac{1-p}{n} \\Big )\\)\nIn matrix form, this can be written as:\n\\[P = pR^{+}A + z1_{n}^T\\]\nwhere, \\(R = \\text{diag}(r_{1},...,r_{n})\\) and \\(z_{j} = \\mathbb{1}(r_{i}>0)(\\frac{1-p}{n}) + \\mathbb{1}(r_{i}=0)(\\frac{1}{n})\\)\nGiven this Markov Chain setup, we can find the stationary distribution which corresponds to the ranks (probabilities) of each page.\nIt turns out that P is a positive row stochastic matrix and thus \\(P^T\\) is column stochastic. The Perron-Frobenius Theorem guarantees that any column stochastic matrix has the following properties:\n1 is an eigenvalue of multiplicity one\n1 is the largest eigenvalue, all others are smaller in modulus\nThere exists a unique eigenvector corresponding to 1 that has all positive entries and sums to 1\nThis theorem thus gives us a few interesting ways to look at this problem, yielding several possible methods of solution.\nThe first way to think about this is as an eigen-problem. We want to find the eigenvector corresponding to eigenvalue 1. Since this eigenvalue is multiplicity one then there is only one such eigenvector (up to a constant). That is, we want to find x such that \\(P^Tx = x\\). There are many possible ways to solve such and eigen-problem such as Singular-Value Decomposition(SVD) or iteratively using the Power-Method.\nThe second way to think about this problem is as a linear system of equations. \\(P^Tx = x \\iff (I-P^T)x = 0\\). We can solve such a linear system using ... or iteratively using the Jacobi Method, Gauss-Seidel, or Bi-Conjugate Gradient Methods.\nIn the rest of this document we explore these different methods of solving the Page Rank Problem using a data set of 500 webpages connected to the www.stat.ncsu.edu webpage. (Go Wolfpack!)\nSummary Statistics of the NCSU Data\nThe A.txt data set is a (500x500) adjacency matrix of the top 500 web-pages linking to the www.stat.ncsu.edu website.\nThe total number of edges is equal to \\(\\sum_{i}{\\sum_{j}{a_{ij}}} = 14,102\\).\nThe out degree of the matrix is \\(r_{i} = \\sum_{j}{a_{ij}}\\). The max out degree was 126, and the min was 0.\nThere were 97 dangling nodes.\nThese calculations were performed with the following code:\n\n\n\n\n\nadj.matrix <- read.table(path_to_dataA,header=FALSE,sep = \",\")\nlabels <- read.table(path_to_dataU,header=FALSE,sep = \",\",as.is=TRUE)\nadj.matrix <- as.matrix(t(adj.matrix))\nlabels<-as.matrix(labels)\n\n(number.of.pages <- dim(adj.matrix)[1])\n\n\n[1] 500\n\n(number.of.edges <- sum(adj.matrix))\n\n\n[1] 14102\n\n##  ri- the out degree = sum_j (Aij)\nout.degree <- apply(adj.matrix,1,sum)\n(min(out.degree))\n\n\n[1] 0\n\n(max(out.degree))\n\n\n[1] 126\n\n(number.of.dangling.nodes <- sum(out.degree==0))\n\n\n[1] 97\n\nin.degree <- apply(adj.matrix,2,sum)\n(max(in.degree))\n\n\n[1] 268\n\n(min(in.degree))\n\n\n[1] 1\n\nLet’s quickly take a second to look at the sparsity pattern in the adjacency matrix. This is accomplished using the function fields::image.plot() from the \"fields\" package. The code to make this plot is below.\n\n\n\nWe can see that the majority of the matrix is zeros, thus this is a sparse matrix. This structure can be exploited to speed up the solving of this system. We also see that many pages have nearly identical linking structures.\nSolving the Page Rank Problem\nFor the analyses that follow, we set the \"teleportation\" parameter to be .15, thus p=.85 The first few methods of solution will look at the problem from the perspective of a linear system, the second as an eigen-problem.\nBefore preceding, we need to read-in/create all the necessary variables and matrices. This is handled in the output below.\n\n\n\nUsing a Dense Linear System Solver\nWe are trying solve the linear equation \\((I-P^T)x = 0\\), thus \\(x \\in \\mathcal{N}(I-P^T)\\). The Perron-Frobenius Theorem guarantees that there is a single vector in the \\(\\mathcal{N}(I-P^T)\\), thus we need not worry about uniqueness. QR decomposition is going to make solving this system very easy.\nIf we perform a QR decomposition on \\((I-P^T)^T\\) with rank r, then the first r columns form an orthonormal basis for the column space of \\((I-P^T)^T\\) while the remaining n-r columns form an orthonormal basis for the null space of \\((I-P^T)\\). Since \\((I-P^T)^T\\) is only rank deficient by one, the last column of Q, when properly normalized, will be our solution.\nBelow is my implementation of this method and the first five elements of my solution vector.\n\n\nqr.i.minus.ptt <- qr(t(i.minus.pt))\nQ <- qr.Q(qr.i.minus.ptt)\nsolution.qr <- Q[,500]\nsolution.qr <- solution.qr/sum(solution.qr)\nsolution.qr[1:5]\n\n\n[1] 0.002136784 0.235939467 0.010670053 0.005944397 0.006703392\n\nA very similar method to solving this problem in almost the exact same fashion, would be to use the svd() function. Singular-Value Decomposition also allows us to find a basis for the null space of \\((I-P^T)^T\\), and this can be accomplished with the following code.\n\n\nsvd.P <- svd(i.minus.pt)\nsolution.svd <- svd.P$v[,500,drop=F]\nsolution.svd <- solution.svd/sum(solution.svd)\nsolution.svd[1:5]\n\n\n[1] 0.002136784 0.235939467 0.010670053 0.005944397 0.006703392\n\nUsing a Simple Iterative Linear System Solver\nNow we want to solve for the same vector as before, using an iterative solver. A very straight-forward method for doing this is the Jacobi method.\nBelow is my implementation of the Jacobi Method for this problem.\n\n\nJacobiSolver <- function(matrixA,b,init.guess,tol,max.iter=5000){\n  ####################################################################\n  ##  Want to solve linear system Ax = b using iterative Jacobi method\n  ##  Input:\n  ##  init.guess = best initial guess for x\n  ##  tol = the tolerance for convergence (looking at max difference)\n  ##  max.iter = stopping condition if convergence not reached\n  ##  Output:\n  ##  solution x with information about convergence\n  ####################################################################\n  if(prod(diag(matrixA)!=0)==0){\n    print(\"All diagonal elements must be non-zero\")\n    return(0)\n  }\n  L.plus.U <- matrixA\n  D <- diag(matrixA)\n  diag(L.plus.U) <- 0\n\n  x.old <- init.guess  ## Initial Conditions\n  x.ten <- init.guess\n  max.iterations <- max.iter\n  tolerance <- tol\n  count = 0\n  for(i in 1:max.iterations){\n    ##  Update\n    x.new <- -1*(1/D)*L.plus.U%*%x.old + (1/D)*b\n\n    ##  Check if converging\n    if(i%%10 == 0){\n      diff <- abs(x.new-x.ten)\n      x.ten <- x.new\n      if(max(diff)<tolerance){ print(noquote(\"Converged!\"));break;}\n    }\n\n    ##  Get ready for the next loop\n    x.old <- x.new\n    count <- count +1\n    if(i == max.iterations){print(\"Not-Converged\");return(0)}\n  }\n  solution.Jacobi <- x.new/sum(x.new)\n  print(noquote(paste(\"Iterative proceedure looped\",count,\"number of times.\")))\n  return(solution.Jacobi)\n}\n##  Make sure it works!\nx.old <- matrix(1/n,nrow=500,ncol=1)\nsolution.Jacobi <- JacobiSolver(i.minus.pt,rep(0,500),x.old,.000001)\n\n\n[1] Converged!\n[1] Iterative proceedure looped 29 number of times.\n\nsolution.Jacobi[1:5]\n\n\n[1] 0.002136784 0.235939467 0.010670053 0.005944397 0.006703392\n\nUsing a Dense Eigen-Solver\nAs an eigen-problem we are trying to find the eigen-vector associated with the maximum eigenvalue 1, which is of multiplicity 1. One great way to do this in \\(\\mathbb{R}\\) is with the eigen() function, which implements Full Symmetric Eigen-Decomposition (tri-diagonalization + QR with implicit shift). This function finds all eigenvalues of a matrix, and on request, will find the eigenvectors as well.\nFor this problem, we know we want to find the eigenvector corresponding to the eigenvalue 1. That is, we are trying to find x such that \\(P^Tx=x\\). This is accomplished with the following code.\n\n\nsolution <- abs(eigen(t(P),symmetric=FALSE)$vectors[,1,drop=F])\nsolution <- solution/sum(solution)\nsolution[1:5]\n\n\n[1] 0.002136784 0.235939467 0.010670053 0.005944397 0.006703392\n\nUsing a Simple Iterative Eigen-Solver\nThe Power Method is a very simple algorithm for finding the maximum eigenvalue and eigenvector for a matrix. We know by the Perron-Frobenius Theorem that 1 is the largest eigenvalue, thus applying this method to \\(P^T\\) will provide the solution to our problem.\nMy implementation of the Power Method for \\(P^T\\) is below.\n\n\nPowerMethodSolver <- function(matrixA,init.guess,tol,max.iter=5000){\n  ####################################################################\n  ##  Want to find large eigenvalue and associated eigenvector\n  ##  Ax = LAMBDAx\n  ##  Input:\n  ##  init.guess = best initial guess for x\n  ##  tol = the tolerance for convergence (looking at max difference)\n  ##  max.iter = stopping condition if convergence not reached\n  ##  Output:\n  ##  solution x with information about convergence\n  ####################################################################\n  x.old <- init.guess  ## Initial Conditions\n  x.ten <- x.old\n  max.iterations <- max.iter\n  tolerance <- tol\n  count = 0\n\n  for(i in 1:max.iterations){\n\n    ##  Calculate next iteration\n    x.new <- matrixA%*%x.old\n    x.new <- x.new/sqrt(sum(x.new^2))\n\n    ##  Check if converging\n    if(i%%10 == 0){\n      diff <- abs(x.new-x.ten)\n      x.ten <- x.new\n      if(max(diff)<tolerance){ print(\"Converged!\");break;}\n    }\n\n    ##  Update\n    x.old <- x.new\n    count <- count+1\n    if(i == max.iterations){print(\"Not-Converged\");return(0)}\n\n  }\n  solution.power <- x.new/sum(x.new)\n  print(noquote(paste(\"Method iterated\",count,\"number of times.\")))\n  return(solution.power)\n}\n##  Make sure it works!\nx.old <- matrix(1/n,nrow=500,ncol=1)  ## Initial Conditions\nsolution.power <- PowerMethodSolver(t(P),x.old,.000001)\n\n\n[1] \"Converged!\"\n[1] Method iterated 59 number of times.\n\nsolution.power[1:5]\n\n\n[1] 0.002136784 0.235939385 0.010670055 0.005944398 0.006703393\n\nThe Results\nNow that we have solved this problem in a number of different ways, we can answer the question that we set out to answer; what are the top 20 websites connected to the www.stat.ncsu.edu website?\nWe can find these websites with the highest stationary state probabilities with the following code.\n\n      Stationary Probabilities\n [1,] \"0.235939466951902\"     \n [2,] \"0.0211065334273055\"    \n [3,] \"0.0144917606322821\"    \n [4,] \"0.0140769056962011\"    \n [5,] \"0.0135504522670688\"    \n [6,] \"0.0110037883588661\"    \n [7,] \"0.010670053024285\"     \n [8,] \"0.0101356734130066\"    \n [9,] \"0.00975252510864476\"   \n[10,] \"0.00823415213171245\"   \n[11,] \"0.0078921509700482\"    \n[12,] \"0.00731078337779315\"   \n[13,] \"0.0069518473720118\"    \n[14,] \"0.00687005153079194\"   \n[15,] \"0.00670339199514352\"   \n[16,] \"0.00623291067709522\"   \n[17,] \"0.00594439722646046\"   \n[18,] \"0.00547885860901607\"   \n[19,] \"0.00534747062763832\"   \n[20,] \"0.00521543243231812\"   \n      Websites                                 \n [1,] \"http://www\"                             \n [2,] \"http://purl.org/rss/1.0/modules/content\"\n [3,] \"http://ns.adobe.com/xap/1.0\"            \n [4,] \"http://ncsu.edu\"                        \n [5,] \"http://instagram.com/ncstate\"           \n [6,] \"http://www.\"                            \n [7,] \"http://www.ncsu.edu\"                    \n [8,] \"http://purl.org/dc/elements/1.1\"        \n [9,] \"http://www.statistics2013.org\"          \n[10,] \"http://drupal.org)\"                     \n[11,] \"http://gmpg.org/xfn/11\"                 \n[12,] \"http://ogp.me/ns\"                       \n[13,] \"http://www.rsscse.org.uk\"               \n[14,] \"http://purl.org/dc/terms\"               \n[15,] \"http://www.lib.ncsu.edu\"                \n[16,] \"http://www.flickr.com/photos/ncsu_scrc\" \n[17,] \"http://www.ncsu.edu/directory\"          \n[18,] \"http://mypack.ncsu.edu\"                 \n[19,] \"http://www.ncsu.edu/campus_map\"         \n[20,] \"http://schema.org/Library\"              \n\nExtension of this Analysis to the Entire Internet\nThe internet today contains some 43.5 Billion webpages. To solve the Page Rank problem with today’s computational power is really only possible using iterative methods that can make use of \"warm starts\", where solutions from previous days runs can be used as good starting values, significantly reducing the necessary number of iterations for convergence.\n\n\n\n",
    "preview": "posts/2021-01-22-the-page-rank-algorithm/the-page-rank-algorithm_files/figure-html5/MakePlot-1.png",
    "last_modified": "2021-01-22T14:22:24-08:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-21-ranking-of-baseball-teams-a-model-based-approach/",
    "title": "Ranking Baseball Teams - A Model Based Approach",
    "description": "In this post we work out the mathematics and implement two methods for fitting\nsimple unstructured Bradley-Terry models from scratch.",
    "author": [
      {
        "name": "Logan Lossing",
        "url": {}
      }
    ],
    "date": "2021-01-21",
    "categories": [
      "R",
      "python",
      "linear algebra",
      "calculus",
      "probability"
    ],
    "contents": "\n\nContents\nThe Model\nDeriving a Minorization-Maximization Algorithm for Maximizing \\(L(\\gamma)\\)\nImplementaion of the MM Algorithm\n\nReparameterization and Newton’s Scoring Method\nImplementation of Newton’s Method\n\nResults\n\nOriginally created in 2014\nIn this post, we wish to use Fisher’s Scoring Method and the MM algorithm to fit a Bradley-Terry model in an effort to rank the baseball teams of the American League. An interesting property of this model is its simplicity; only information about which teams have played each other in the regular season and, for each game played, noting which team won is used in the ranking.\nThe data used to illustrate the implementation of these algorithms in this post consists of the regular season match records of every team in the American League in the MLB for the 2013. For simplicity, games against National league opponents are ignored.\nThe Model\nConsider a league of p teams (p=15 for the our example), where each team has a strength parameter \\(\\gamma_i >0\\). Team \\(i\\) beats team \\(j\\) with probability \\(\\frac{\\gamma_i}{\\gamma_i + \\gamma_j}\\). Let \\(y_{ij}\\) represent the number of times that team \\(i\\) beats team \\(j\\), and for convenience we take \\(y_{ii} = 0\\), i = 1,2,...,p. If we assume independence between games then the log-likehood of this model is:\n\\[L(\\gamma) = \\sum_{i,j}{y_{ij}\\left [ \\ln(\\gamma_i) - \\ln(\\gamma_i + \\gamma_j) \\right]}\\]\nIs the log-likelihood concave?\nTo determine the concavity of the log-likelihood function, we will take a look at the Hessian. Expanding the log-likelihood a little helps with taking the derivatives. Doing so we obtain,\n\\[L(\\gamma) = y_{12}(\\ln\\gamma_1 - \\ln(\\gamma_1 + \\gamma_2)) + y_{13}(\\ln\\gamma_2 - \\ln(\\gamma_1 + \\gamma_3)) + ... + y_{p(p-1)}(\\ln\\gamma_p - \\ln(\\gamma_{p} + \\gamma_{p-1}))\\]\nThus the gradient is given by:\n\\[\\nabla L(\\gamma)_k = \\left \\{ \\sum_{i=1}^p{y_{ik} \\left ( \\frac{1}{\\gamma_i} - \\frac{1}{\\gamma_i + \\gamma_k} \\right ) } - \\sum_{i=1}^p{y_{ki} \\left ( \\frac{1}{\\gamma_i + \\gamma_k} \\right )} \\right \\}, \\quad \\text{for} \\quad k = 1,2,...,p\\]\nAgain, careful differentiation yield the Hessian given by:\n\\[\\nabla^2L(\\gamma)_{lk} = \n\\begin{cases}\n\\sum_{j=1}^p{y_{lj}\\left [{ \\frac{-1}{\\gamma_l^2} + \\frac{1}{(\\gamma_l + \\gamma_j)^2}}\\right]} + \\sum_{j=1}^p{y_{jk}(\\frac{1}{(\\gamma_j + \\gamma_k)^2})}, & k = l \\\\\n(y_{lk} + y_{kl})\\frac{1}{(\\gamma_l + \\gamma_k)^2}, & k \\neq l\n\\end{cases}\\]\nNow, is the log-likelihood concave?\nNo, it does not appear to be so. To see this consider the scenario where one team is particularly bad and fails to win any games against any opponents in a given year. If this is the case, the Observed Information Matrix will have a negative on the diagonal and thus the likelihood cannot be concave.\nDeriving a Minorization-Maximization Algorithm for Maximizing \\(L(\\gamma)\\)\nFor this section, we want to find a minorizing function \\(g(\\gamma|\\gamma^{(t)})\\) that has the property that \\(g(\\gamma^{(t)}|\\gamma^{(t)}) = L(\\gamma^{(t)})\\) at \\(\\gamma = \\gamma^{(t)}\\) and otherwise, for all \\(\\gamma\\) we have that \\(g(\\gamma|\\gamma^{(t)}) < L(\\gamma)\\). These conditions are referred to as dominance and tangency conditions.\nThe supporting hyperplane inequality states that a differentiable function \\(f(\\cdot)\\) is convex in U \\(\\iff\\) \\(f(x) \\ge f(y) + \\langle \\nabla f(y), x-y \\rangle\\) for all \\(x,y \\in U\\).\nCondider the function \\(f(x) = -\\ln(x)\\) a convex function. Thus the supporting hyperplane inequality yields \\(-\\ln(x) \\ge -\\ln(y) + \\frac{-(x-y)}{x}\\). Letting \\(x = \\gamma_i + \\gamma_j\\) and \\(y = \\gamma_i^{(t)} + \\gamma_j^{(t)}\\) then we obtain:\n\\[\\begin{aligned}\n-\\ln(\\gamma_i + \\gamma_j) &\\ge -\\ln(\\gamma_i^{(t)} + \\gamma_j^{(t)}) + \\frac{-(\\gamma_i + \\gamma_j-\\gamma_i^{(t)} + \\gamma_j^{(t)})}{\\gamma_i^{(t)} + \\gamma_j^{(t)}} \\\\\n&= -\\frac{\\gamma_i + \\gamma_j}{\\gamma_i^{(t)} + \\gamma_j^{(t)})} + c(t) \\quad \\text{where c(t) is a constant with regard optimization}\\end{aligned}\\]\nNote, that the above inequality yield strict inequality when \\(\\gamma \\neq \\gamma^{(t)}\\) and equality precisely when \\(\\gamma = \\gamma^{(t)}\\).\nPlugging this expression into the log-likelihood from above we obtain an expression for the minorizing function that satisfies both properties listed above: dominance and tangency at \\(\\gamma^{(t)}\\). This minorizing function is given below.\n\\[\\begin{aligned}\ng(\\gamma|\\gamma^{(t)}) &= \\sum_{i,j}{y_{ij}\\left [ \\ln(\\gamma_i) -\\ln(\\gamma_i^{(t)} - \\gamma_j^{(t)}) + \\frac{-(\\gamma_i + \\gamma_j-\\gamma_i^{(t)} - \\gamma_j^{(t)})}{\\gamma_i^{(t)} + \\gamma_j^{(t)}} \\right]} \\\\\n&= \\sum_{i,j}{y_{ij}\\left [ \\ln(\\gamma_i)  + \\frac{-(\\gamma_i + \\gamma_j)}{\\gamma_i^{(t)} + \\gamma_j^{(t)}}  + c(t) \\right]} \\end{aligned}\\]\nThe benefit here is that the MM updates can be calculated analytically and thus computed with incredible efficiency.\nTo do this we first differentiate to obtain:\n\\[\\nabla g(\\gamma)_k = \\sum_{j=1}^p{y_{kj} \\left (\\frac{1}{\\gamma_k} - \\frac{1}{\\gamma_k^{(t)} + \\gamma_j^{(t)}} \\right )} - \\sum_{j=1}^p{y_{jk} \\left (\\frac{1}{\\gamma_k^{(t)} + \\gamma_j^{(t)}} \\right )}\\]\nSetting this equal to zero and solving for \\(\\gamma_k\\) then we have our updates.\n\\[\\gamma_k^{(t+1)} = \\frac{\\sum_{j=1}^p{y_{kj}}}{\\sum_{j=1}^P{\\left [ \\frac{y_{jk} + y_{kj}}{\\gamma_k^{(t)} + \\gamma_j^{(t)}}\\right ]}}\\]\nImplementaion of the MM Algorithm\nThe following is my code implementing the MM algorithm for these data. This algorithim is amazingly simple and efficient for these data. First it is necessary to read in the data and create a few functions to allow the main routine to be uncluttered. Also note, the parameters of this model are not identifiable without some remedy which was, in this case, to set \\(\\gamma_1 = 1\\).\nFirst, we need to get the data from ESPN, we can scrape the standings using the following Python routine:\n\nfrom bs4 import BeautifulSoup\nimport urllib2\nimport re\nimport csv\n\ncsv_filename = 'AL-standings.csv'\n\nyear = '2013'\nurl = 'http://espn.go.com/mlb/standings/grid/_/year/' + year\n\npage = urllib2.urlopen(url)\nsoup = BeautifulSoup(page.read())\n\n# Extracts the table for the American League (AL) and the rows for each team\nAL_table = soup.find(text = re.compile(\"American\")).find_parent(\"table\")\nAL_rows = AL_table.findAll('tr', class_ = re.compile(\"team\"))\n\n# Creates a list of the AL teams and then appends NL for National League\nAL_teams = [team_row.find('b').text for team_row in AL_rows]\nAL_teams.append(\"NL\")\n\n# Opens a CSV file for the AL standings\nwith open(csv_filename, 'wb') as f:\n    csv_out = csv.writer(f)\n    csv_out.writerow(['Team', 'Opponent', 'Wins', 'Losses'])\n    \n    # For each team in the AL table, identifies the team's name, the opponent,\n    # and their wins and losses (WL) against that opponent. Then outputs the\n    # results to the open CSV file\n    for team_row in AL_rows:\n        team = team_row.find('b').text\n        \n        # A cell has the following form:\n        # <td align=\"right\">\n        # 7-9<\/td>\n        WL_cells = team_row.findAll('td', align = \"right\")\n        \n        # Extracts the values for both wins and losses from each WL table cell\n        wins_losses = [td_cell.text.strip('\\n').split('-') for td_cell in WL_cells]\n        \n        # Writes the current team's standings to the CSV file\n        for i, opponent in enumerate(AL_teams):\n            if team != opponent:\n                csv_out.writerow([team, opponent, wins_losses[i][0], wins_losses[i][1]])\n\nAfter writing the data to csv, we then read the data back in with R to finish the analysis. In the code below, you can see we have square matrix with each row representing a team in the AL and each column representing each of the possible AL opponents.\nFor a fixed row, the number in a specific column corresponds to the number of games won vs the opponent listed with that column label. Example: In the first row, second column we see 11. This means that the BAL(timore) Orioles beat the BOS(ton) Red Sox 11 times during regular season play. The entry in row 2, column 1 is an 8, meaning that the BOS(ton) Red Sox beat the BAL(timore) Orioles 8 times during regular season play. No other data is used for this analysis. (The results are suprisingly good for such a simple model!)\n\n\n## Import Data\npath_to_data <- \"./ymat.csv\"\nymat <-read.csv(path_to_data, header = TRUE, as.is = TRUE)\nhead(ymat)\n\n\n    BAL BOS CHW CLE DET HOU KC LAA MIN NYY OAK SEA TB TEX TOR\nBAL   0  11   4   3   4   4  3   5   3   9   5   2  6   5  10\nBOS   8   0   4   6   3   6  2   3   4  13   3   6 12   2  11\nCHW   3   2   0   2   7   3  9   3   8   3   2   3  2   4   4\nCLE   4   1  17   0   4   6 10   4  13   1   5   5  2   5   4\nDET   2   4  12  15   0   6  9   0  11   3   3   5  3   3   5\nHOU   2   1   4   1   1   0  2  10   1   1   4   9  2   2   3\n\nymat <- apply(ymat,1,as.numeric)\nrownames(ymat) <- colnames(ymat)\nymat <- t(ymat)\n\n##############################################################################\n##  Calculating the likelihood\nsum.matrix.product.diagonals <- function(matA,matB){\n  ## Two nxn matrices, only forms the diagonal entries of the product\n  len <- dim(matA)[1]\n  return(sum(sapply(1:len,function(i) matA[i,]%*%matB[,i])))\n}\nloglikelihood.gamma <- function(gam,dat){\n  ##  Inputs:  parameter vector gam and data matrix dat\n  ##  Returns: scalar value of the log-likelihood\n  len <- length(gam)\n  entries <- outer(1:len,1:len,function(i,j) gam[i]/(gam[i]+gam[j]))\n  log.matrix <- log(entries)\n  return(sum.matrix.product.diagonals(log.matrix,t(dat)))\n}\n##############################################################################\n## Calculating the Gradient\ngradient.loglik <- function(gam,dat){\n  len <- length(gam)\n  mat1 <- outer(1:len,1:len,function(i,j) gam[i]/(gam[i]*(gam[i]+gam[j])))*dat\n  mat2 <- t(outer(1:len,1:len,function(i,j) 1/(gam[i]+gam[j])))*dat\n  gradient <- colSums(mat1) - colSums(mat2)\n  return(gradient)\n}\n##############################################################################\n## MM Update\nMM.compute.gam.update<- function(gam,dat){\n  ## Given the current value of the gam at time t, and the data matrix\n  ## Calcumates the the updated value for t+1, returns a vector\n  num <- rowSums(dat)\n  denom <- rowSums((dat+t(dat))/outer(gam,gam,\"+\" ))\n  updateRank <- num / denom\n  return(updateRank) \n}\n######################################################################\n##  Good Starting Values\ninitial.estimate <- function(dat){\n  ## Take the win% of all games played as a reasonable starting point\n  gamma.MM <- rowSums(dat)/(rowSums(dat) + colSums(dat))\n  gamma.MM <- gamma.MM/gamma.MM[1]\n  return(gamma.MM)\n}\n\n\n\nNow that all the setup functions have been loaded we can bring in the main routine that actually will return the estimated strengths for each team in the American League.\n\n\nstrength.fit.MM <- function(dat, gamma0 = NULL,maxiters = 1000,tolfun = 1e-6){\n\n  if(is.null(gamma0)){\n    gamma.t <- initial.estimate(dat)\n  } else{gamma.t <- gamma0}\n  \n  for(i in 1:maxiters){\n    \n    gamma.tp1 <- MM.compute.gam.update(gamma.t,dat) ##  Update\n    gamma.tp1 <- gamma.tp1/gamma.tp1[1] ## Normalize for identifiability\n    \n    loglik.tp1 <- loglikelihood.gamma(gamma.tp1,dat)  ## loglik at update\n    loglik.tp <- loglikelihood.gamma(gamma.t,dat) ## loglik at previous\n    improve.lik <- abs(loglik.tp1 - loglik.tp) ## Amount of improvement\n    \n    if(improve.lik<tolfun){ ## Have I converged?\n      gradient <- gradient.loglik(gamma.tp1,dat)\n      return(list(\"Strengths\" = gamma.tp1, \"Iterations\" = i,\n        \"Maximum\" = loglik.tp1,\"Gradient\" = gradient))\n    } else{ gamma.t <- gamma.tp1} ## If not, CONTINUE TO ITERATE\n    \n  } # end for\n  print(\"No Convergence\")\n  return(gamma.tp1)\n}\nranking <- strength.fit.MM(ymat)\nranking\n\n\n$Strengths\n      BAL       BOS       CHW       CLE       DET       HOU        KC \n1.0000000 1.2510850 0.5642156 1.1030067 1.1009506 0.3857875 0.9993401 \n      LAA       MIN       NYY       OAK       SEA        TB       TEX \n0.7532999 0.6160583 1.0492604 1.1049180 0.6650877 1.1384163 1.0401907 \n      TOR \n0.7518629 \n\n$Iterations\n[1] 18\n\n$Maximum\n[1] -711.5312\n\n$Gradient\n          BAL           BOS           CHW           CLE           DET \n 0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00 \n          HOU            KC           LAA           MIN           NYY \n 0.000000e+00  7.105427e-15 -7.105427e-15  0.000000e+00 -7.105427e-15 \n          OAK           SEA            TB           TEX           TOR \n 0.000000e+00  7.105427e-15  0.000000e+00  0.000000e+00  0.000000e+00 \n\nIn the results section, the results of the analysis will be described in detail. Now it is of interest to perform the exact same analysis using a Newton Algorithim.\nReparameterization and Newton’s Scoring Method\nFor this section, consider the reparameterization \\(\\gamma_i = e^{\\lambda_i}\\) for \\(\\lambda \\in \\mathbb{R}\\). The log-likelihood now becomes:\n\\[L(\\lambda) = \\sum_{i,j}{y_{ij}\\left [ \\lambda_i - \\ln(e^{\\lambda_i} + e^{\\lambda_i}) \\right]}\\]\nDifferentiating yields the following gradient:\n\\[\\nabla L(\\lambda)_k = \\sum_{j=1}^k{y_{kj} \\left ( 1 - \\frac{e^{\\lambda_k}}{e^{\\lambda_k + \\lambda_j}} \\right )} + \\sum_{j=1}^p{y_{jk}\\left ( \\frac{-e^{\\lambda_k}}{e^{\\lambda_k} + e^{\\lambda_j} }\\right)}\\]\nThe Hessian is given by:\n\\[\\nabla^2 L(\\lambda)_{kj} = \n\\begin{cases}\n-\\left [  \\sum_{j=1}^p{(y_{kj} + y_{jk})\\left ( \\frac{e^{\\lambda_k + \\lambda_j}}{ (e^{\\lambda_j} + e^{\\lambda_k})^2} \\right )} \\right ], & k = j \\\\\n\\frac{(y_{kj} + y_{jk})e^{\\lambda_j + \\lambda_k}}{(e^{\\lambda_j} + e^{\\lambda_k})^2}, & k \\neq j\n\\end{cases}\\]\nFor this reparameterized likelihood, the resulting Hessian matrix is negative definite and thus the log-likelihood under this representation is concave. This result is due to the closure of log-convex functions under addition (see \"Convex Optimization\"-Stephen Boyd pg.105) Further, since we have reparameterized our parameters to the real line, there is no need to worry about backtracking in the implementation of Newton’s Method.\nIt is now time to obtain the estimates of our parameters using Newton’s Method. The implementation of this method is below.\nImplementation of Newton’s Method\nThis method has quite a few more \"moving parts\" than did the MM implementation. Not only does the likelihood need to be computed, but also the gradient and Hessian matrices. The set-up for the actual main routine is below.\n\n\nloglikelihood.lambda <- function(lam,dat){\n  len <- length(lam)\n  entries <- outer(1:len,1:len,function(i,j) lam[i]-log(exp(lam[i])+exp(lam[j])) )\n  log.lik <- sum(dat*entries)\n  return(log.lik)\n}\n##############################################################################\n## Good starting point\ninitial.estimate.lambda <- function(dat){\n  lambda.Newt <- log(rowSums(dat)/(rowSums(dat)+colSums(dat)))\n  lambda.Newt <- lambda.Newt - lambda.Newt[1]\n  return(lambda.Newt) \n}\n###############################################################################\n## Gradient\ngradient.loglik.lam <- function(lam,dat){\n  len <- length(lam)\n  mat1 <- outer(1:len,1:len,function(i,j) exp(lam[i])/(exp(lam[i]) + exp(lam[j])))\n  mat1 <- mat1*(dat + t(dat))\n  gradient <- rowSums(dat) - rowSums(mat1)\n  return(gradient)\n}\n###############################################################################\n## Observed Fisher Information Matrix\nobs.info.lam <- function(lam,dat){\n  len <- length(lam)\n  mat1 <- outer(1:len,1:len,\n    function(i,j) exp(lam[i]+lam[j])/(exp(lam[i])+exp(lam[j]))^2)*(dat + t(dat))\n  diagonals <- rowSums(mat1)\n  mat1 <- -1*mat1\n  diag(mat1) <- diagonals\n  return(mat1)\n}\n#############################################################################\n## Modified Cholesky Decomposition, creates positive definite approximation\nmodifiedCholeskyUpdate <- function(matrixA,matrixL.km1){\n  ############################################################\n  ##  Inputs:  matrixA- is the KxK matrix we are decomposing\n  ##           matrixL.km1- the K-1xK-1 lower triangular matrix\n  ##                        obtained from previous iterations\n  ##  Outputs: matrixL.k- the KxK lower tri matrix\n  #############################################################\n  \n  dimension <- dim(matrixA)[1]\n  \n  a.k.vector <- matrixA[-dimension,dimension,drop=F]\n  a.kk <- matrixA[dimension,dimension,drop=F]\n  \n  l.k.vector <- forwardsolve(matrixL.km1,a.k.vector,k=dimension-1)\n  check <- a.kk - t(l.k.vector)%*%l.k.vector\n  if(check >0){\n    l.kk <- sqrt(a.kk - t(l.k.vector)%*%l.k.vector)\n  }else{l.kk <- .01}\n  \n  matrixL.k <-cbind(matrixL.km1,rep(0,dimension-1))\n  last.row <- c(l.k.vector,l.kk)\n  matrixL.k <- rbind(matrixL.k,last.row)\n  matrixL.k <- as.matrix(matrixL.k)\n  rownames(matrixL.k) <- NULL\n  colnames(matrixL.k) <- NULL\n  return(matrixL.k)\n}\nmodifiedCholeskyDecomp <-function(matrixA){\n  ##################################################################\n  ##  Inputs:  matrixA- is the nxn matrix we are decomposing     \n  ##  Outputs: L- the nxn lower tri matrix such that matrixA = LL^t\n  ##################################################################\n  \n  dimension <- dim(matrixA)\n  if(dimension[1] != dimension[2]){return(Warning = \"Non-Square Matrix\")}\n  if(!isSymmetric(matrixA)){return(Warning = \"NOT SYMMETRIC\")}\n  \n  A2 <- matrixA[1:2,1:2,drop=F]\n  \n  if(as.numeric(matrixA[1,1]) >= 0){\n    L1 <- sqrt(matrixA[1,1,drop=F])\n  }else{ L1 <- .01}\n  \n  for(i in 1:(dimension[1]-1)){\n    L1 <- modifiedCholeskyUpdate(A2,L1)\n    if(i<dimension[1]-1){\n      A2 <- matrixA[1:(i+2),1:(i+2),drop = F]\n    }\n  }\n  return(L = L1)\n}\n\n\n\nNow that all the necessary functions have been created, we can call the main routine and obtain some estimates. This implementation is below.\n\n\nstrength.fit.Newton <- function(dat,lambda0=NULL,maxiters=100,tolfun=1e-6){\n  \n  if(is.null(lambda0)){\n    lambda.t <- initial.estimate.lambda(dat)\n  } else{lambda.t <- lambda0}\n  len <- length(lambda.t)\n  \n  for(i in 1:maxiters){\n    \n    gradient.t <- gradient.loglik.lam(lambda.t,dat) \n    lower <- modifiedCholeskyDecomp(obs.info.lam(lambda.t,dat)) ## Find dir to ascend\n    l.inverse.times.gradient <- forwardsolve(lower,gradient.t)\n    x <- backsolve(t(lower),l.inverse.times.gradient)\n    \n    lambda.tp1 <- lambda.t + x  ## Get update\n    lambda.tp1 <- lambda.tp1-lambda.tp1[1]  ## Normalize\n    \n    loglik.tp1 <- loglikelihood.lambda(lambda.tp1,dat)  ## loglik at update\n    loglik.tp <- loglikelihood.lambda(lambda.t,dat) ## loglik at previous\n    improve.lik <- abs(loglik.tp1 - loglik.tp)  ## Improvement in likelihood\n    \n    if(improve.lik<tolfun){  ## Have we CONVERGED?\n      gradient <- gradient.loglik.lam(lambda.tp1,dat)\n      return(list(\"Strengths\" = lambda.tp1, \"Iterations\" = i,\"Maximum\" = loglik.tp1,\n              \"Gradient\" = gradient))\n    } else{ lambda.t <- lambda.tp1}\n    \n  } # end for\n  print(\"No Convergence\")\n  return(lambda.tp1)\n}\nstrengths <- strength.fit.Newton(ymat)\nstrengths\n\n\n$Strengths\n          BAL           BOS           CHW           CLE           DET \n 0.0000000000  0.2240370238 -0.5724141333  0.0979381904  0.0960691213 \n          HOU            KC           LAA           MIN           NYY \n-0.9525885657 -0.0007575662 -0.2834240107 -0.4845092637  0.0480898408 \n          OAK           SEA            TB           TEX           TOR \n 0.0996121698 -0.4079668384  0.1296495476  0.0392569072 -0.2852165434 \n\n$Iterations\n[1] 3\n\n$Maximum\n[1] -711.5312\n\n$Gradient\n          BAL           BOS           CHW           CLE           DET \n 1.697572e-09  2.494630e-09 -3.477396e-11  2.190689e-09  2.179974e-09 \n          HOU            KC           LAA           MIN           NYY \n-3.202133e-08  1.669122e-09  3.090079e-09  4.267733e-10  1.770061e-09 \n          OAK           SEA            TB           TEX           TOR \n 5.669420e-09  2.072611e-09  2.230038e-09  5.205393e-09  1.359737e-09 \n\nLook! We have obtained the same results as before and in very few iterations, the strength of this method.\nWhat Structure of the Hessian Could be Exploited When P is Very Large?\nIf this method was to be applied to a very large tournament, then it would be likely that most competitors would only play a tiny fraction of the possible opponents. This would result in a very sparse Hessian matrix whose structure could be exploited using Newton’s Method.\n\nResults\nNow that we have investigated how to obtain estimates for the strength parameters in our model, what do they tell us about the American League in MLB baseball?\nTeam\nStrength\nWin%\nTotal Wins\nBOS\n1.25\n0.58\n83.00\nTB\n1.14\n0.56\n80.00\nOAK\n1.10\n0.58\n83.00\nCLE\n1.10\n0.57\n81.00\nDET\n1.10\n0.57\n81.00\nNYY\n1.05\n0.54\n76.00\nTEX\n1.04\n0.57\n81.00\nBAL\n1.00\n0.52\n74.00\nKC\n1.00\n0.54\n77.00\nLAA\n0.75\n0.48\n68.00\nTOR\n0.75\n0.44\n63.00\nSEA\n0.67\n0.44\n63.00\nMIN\n0.62\n0.41\n58.00\nCHW\n0.56\n0.39\n55.00\nHOU\n0.39\n0.30\n43.00\nFirst consider the table above. The “Strength\" column shows us, from best to worst, the rankings provided by this model via estimation of the strength parameters. The”Win%\" column shows each teams actual percentage of games won. It is interesting to note the differences in ranking and actual winning%.\nThis discrepancy appears to be related to certain divisions in the American League having more strong teams than others. Since teams within a division play more, this model adjusts for playing better opponents. It is also interesting to note that the Boston Red Sox are ranked number one in this model and would actually go on to win the World Series in 2013.\nSince we have a way to turn our strength parameter estimates into probailities, the table below shows the probability any team in the AL would win in a match against any other opponent. The way to read the table is to pick a row, and then the numbers in that row are the probability that the team indicated by the row would beat the team in the column.\n\nBAL\nBOS\nCHW\nCLE\nDET\nHOU\nKC\nLAA\nMIN\nNYY\nOAK\nSEA\nTB\nTEX\nTOR\n\nBAL\n\n0.44\n0.64\n0.48\n0.48\n0.72\n0.50\n0.57\n0.62\n0.49\n0.47\n0.60\n0.47\n0.49\n0.57\n\nBOS\n0.56\n\n0.69\n0.53\n0.53\n0.76\n0.56\n0.62\n0.67\n0.54\n0.53\n0.65\n0.52\n0.55\n0.62\n\nCHW\n0.36\n0.31\n\n0.34\n0.34\n0.59\n0.36\n0.43\n0.48\n0.35\n0.34\n0.46\n0.33\n0.35\n0.43\n\nCLE\n0.52\n0.47\n0.66\n\n0.50\n0.74\n0.53\n0.59\n0.64\n0.51\n0.50\n0.62\n0.49\n0.52\n0.59\n\nDET\n0.52\n0.47\n0.66\n0.50\n\n0.74\n0.52\n0.59\n0.64\n0.51\n0.50\n0.62\n0.49\n0.51\n0.59\n\nHOU\n0.28\n0.24\n0.41\n0.26\n0.26\n\n0.28\n0.34\n0.39\n0.27\n0.26\n0.37\n0.25\n0.27\n0.34\n\nKC\n0.50\n0.44\n0.64\n0.47\n0.48\n0.72\n\n0.57\n0.62\n0.49\n0.47\n0.60\n0.47\n0.49\n0.57\n\nLAA\n0.43\n0.38\n0.57\n0.41\n0.41\n0.66\n0.43\n\n0.55\n0.42\n0.41\n0.53\n0.40\n0.42\n0.50\n\nMIN\n0.38\n0.33\n0.52\n0.36\n0.36\n0.61\n0.38\n0.45\n\n0.37\n0.36\n0.48\n0.35\n0.37\n0.45\n\nNYY\n0.51\n0.46\n0.65\n0.49\n0.49\n0.73\n0.51\n0.58\n0.63\n\n0.49\n0.61\n0.48\n0.50\n0.58\n\nOAK\n0.53\n0.47\n0.66\n0.50\n0.50\n0.74\n0.53\n0.59\n0.64\n0.51\n\n0.62\n0.49\n0.52\n0.59\n\nSEA\n0.40\n0.35\n0.54\n0.38\n0.38\n0.63\n0.40\n0.47\n0.52\n0.39\n0.38\n\n0.37\n0.39\n0.47\n\nTB\n0.53\n0.48\n0.67\n0.51\n0.51\n0.75\n0.53\n0.60\n0.65\n0.52\n0.51\n0.63\n\n0.52\n0.60\n\nTEX\n0.51\n0.45\n0.65\n0.48\n0.49\n0.73\n0.51\n0.58\n0.63\n0.50\n0.48\n0.61\n0.48\n\n0.58\n\nTOR\n0.43\n0.38\n0.57\n0.41\n0.41\n0.66\n0.43\n0.50\n0.55\n0.42\n0.41\n0.53\n0.40\n0.42\n\n\nFinally, it is always nice to visualize the above table so that we can easily see which teams are good and which teams are not so good. A heat map version of the above table can do just this. The plot below conveys the exact same information but rather than being interested in specific values, we are more interested in the patterns, easily pointing out which teams have performed poorly in the 2013 regular season. For example, the lowest ranked team, the Houston Astros, have a red band all the way across their row, showing a low probability of beating any opponent whereas the Boston Red Sox’s row is mostly blue and purple, showing their dominance over the rest of the league.\nHeat Map of the Estimated Win Probabilities for Teams in the American League\n\n\n",
    "preview": "posts/2021-01-21-ranking-of-baseball-teams-a-model-based-approach/heatMap.png",
    "last_modified": "2021-01-22T14:23:29-08:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-20-welcome/",
    "title": "Welcome",
    "description": "The journey begins...",
    "author": [
      {
        "name": "Logan Lossing",
        "url": {}
      }
    ],
    "date": "2021-01-20",
    "categories": [],
    "contents": "\nTechnology is amazing. Modern frameworks and computing architectures are more powerful than ever. Despite the progress, it can be quite time consuming to learn a new technology, especially if not done in an organized way. To be a little more thoughtful and organized with my own professional development, I decided to start this blog where I can turn my random musings into more polished articles for the benefit of myself and anyone else that finds it useful.\nShout out to all of the amazing developers around the world that share their amazing materials with the world! You all are an inspiration.\nLink to talk by David Robinson - The Unreasonalbe Effectiveness of Public Work\n\n\n\n",
    "preview": {},
    "last_modified": "2021-01-22T14:19:40-08:00",
    "input_file": {}
  }
]
